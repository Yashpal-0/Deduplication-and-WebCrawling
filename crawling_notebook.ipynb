{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crawling Assignment Activity 2.2\n",
        "\n",
        "This notebook interacts with the local crawling assignment server (running at `http://localhost:3000`) to\n",
        "- discover the site graph with minimal page visits,\n",
        "- track `node_id` updates for each page, and\n",
        "- estimate PageRank scores over the discovered link structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Helper Functions\n",
        "\n",
        "The web server returns JSON responses. We use `requests` for HTTP and utilities for crawling and scoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install beautifulsoup4 --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import re\n",
        "from collections import deque, defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Set, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"http://localhost:3000\"\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"CrawlerAssignmentBot/1.2\",\n",
        "    \"Accept\": \"text/html,application/json\",\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'path': '/',\n",
              " 'page_id': 'page_1yi8d2sx',\n",
              " 'node_id': '158pgabnwj9a',\n",
              " 'last_updated': '2025-11-17 19:48:31 UTC',\n",
              " 'history': [],\n",
              " 'links': ['/page_28kxnvap',\n",
              "  '/page_8qmlk6n0',\n",
              "  '/page_ds6a104p',\n",
              "  '/page_ozgkl2gr',\n",
              "  '/page_rqodhkwd']}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def fetch_page(path: str = \"/\") -> str:\n",
        "    url = BASE_URL.rstrip(\"/\") + path\n",
        "    response = session.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def parse_page(content: str, path: str) -> dict:\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "    page_id_text = soup.select_one(\".page-id\")\n",
        "    page_id = \"\"\n",
        "    if page_id_text:\n",
        "        page_id = page_id_text.get_text(strip=True).split(\":\")[-1].strip()\n",
        "\n",
        "    node_id_elem = soup.select_one(\".node-id b\")\n",
        "    node_id = node_id_elem.get_text(strip=True) if node_id_elem else \"\"\n",
        "\n",
        "    last_updated_elem = soup.select_one(\".last-updated\")\n",
        "    last_updated = \"\"\n",
        "    if last_updated_elem:\n",
        "        last_updated = last_updated_elem.get_text(strip=True)\n",
        "        if \":\" in last_updated:\n",
        "            last_updated = last_updated.split(\":\", 1)[-1].strip()\n",
        "\n",
        "    history_entries: List[dict] = []\n",
        "    history_container = soup.select_one(\"details\")\n",
        "    if history_container:\n",
        "        for entry in history_container.select(\"div\"):\n",
        "            text = entry.get_text(strip=True)\n",
        "            text = text.strip(\"\\u0007 \\n\\r\\t\")\n",
        "            match = re.match(r\"([A-Za-z0-9]+)\\s*\\(([^)]+)\\)\", text)\n",
        "            if match:\n",
        "                history_entries.append({\n",
        "                    \"node_id\": match.group(1),\n",
        "                    \"timestamp\": match.group(2),\n",
        "                })\n",
        "\n",
        "    outgoing_links: List[str] = []\n",
        "    for link in soup.select(\"a.file-link\"):\n",
        "        href = link.get(\"href\")\n",
        "        if href and href.startswith(\"/page_\"):\n",
        "            outgoing_links.append(href)\n",
        "    outgoing_links = sorted(set(outgoing_links))\n",
        "\n",
        "    return {\n",
        "        \"path\": path,\n",
        "        \"page_id\": page_id,\n",
        "        \"node_id\": node_id,\n",
        "        \"last_updated\": last_updated,\n",
        "        \"history\": history_entries,\n",
        "        \"links\": outgoing_links,\n",
        "    }\n",
        "\n",
        "\n",
        "root_html = fetch_page(\"/\")\n",
        "root_parsed = parse_page(root_html, \"/\")\n",
        "root_parsed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PageState:\n",
        "    page_id: str\n",
        "    path: str\n",
        "    last_node_id: str\n",
        "    last_updated_at: str\n",
        "    history: List[dict] = field(default_factory=list)\n",
        "    outgoing: List[str] = field(default_factory=list)\n",
        "    last_fetched_ts: float = field(default_factory=time.time)\n",
        "    last_changed_ts: float = field(default_factory=time.time)\n",
        "    updates_detected: int = 0\n",
        "\n",
        "    def differs_from(self, node_id: str, last_updated: str, history: List[dict]) -> bool:\n",
        "        if node_id != self.last_node_id:\n",
        "            return True\n",
        "        if last_updated and last_updated != self.last_updated_at:\n",
        "            return True\n",
        "        if len(history) != len(self.history):\n",
        "            return True\n",
        "        if history and self.history:\n",
        "            return history[-1] != self.history[-1]\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientCrawler:\n",
        "    \"\"\"\n",
        "    Optimized crawler with the following efficiency improvements:\n",
        "    1. Path normalization caching to avoid repeated string operations\n",
        "    2. Seen paths tracking (set) for O(1) duplicate detection vs O(n) queue checks\n",
        "    3. Time caching to reduce system calls\n",
        "    4. Selective logging (only new/changed pages) to reduce memory\n",
        "    5. Optimized refresh: single pass filter + slice instead of break\n",
        "    6. Efficient graph building: set operations instead of unions\n",
        "    7. PageRank optimization: pre-compute sink contributions, separate sink/non-sink processing\n",
        "    8. Single-pass summary calculation\n",
        "    \"\"\"\n",
        "    def __init__(self, seed_path: str = \"/\", revisit_window: float = 120.0):\n",
        "        self.seed_path = seed_path\n",
        "        self.revisit_window = revisit_window\n",
        "        self.pages: Dict[str, PageState] = {}\n",
        "        self.graph: Dict[str, Set[str]] = defaultdict(set)\n",
        "        self.page_visits = 0\n",
        "        self.node_updates = 0\n",
        "        self.fetch_log: List[dict] = []\n",
        "        # Optimization: Track seen paths to avoid duplicate queue entries\n",
        "        self._seen_paths: Set[str] = set()\n",
        "        # Optimization: Cache normalized paths\n",
        "        self._path_cache: Dict[str, str] = {}\n",
        "\n",
        "    def normalize_path(self, path: str) -> str:\n",
        "        # Use cache to avoid repeated normalization\n",
        "        if path in self._path_cache:\n",
        "            return self._path_cache[path]\n",
        "        \n",
        "        original_path = path\n",
        "        if path.startswith(\"http://\") or path.startswith(\"https://\"):\n",
        "            if path.startswith(BASE_URL):\n",
        "                path = path[len(BASE_URL):]\n",
        "            else:\n",
        "                self._path_cache[original_path] = path\n",
        "                return path  # external link\n",
        "        if not path.startswith(\"/\"):\n",
        "            path = \"/\" + path\n",
        "        \n",
        "        self._path_cache[original_path] = path\n",
        "        return path\n",
        "\n",
        "    def enqueue_links(self, path: str, links: List[str], queue: deque):\n",
        "        normalized_links = []\n",
        "        for link in links:\n",
        "            normalized = self.normalize_path(link)\n",
        "            if not normalized.startswith(\"/\"):\n",
        "                continue  # skip external\n",
        "            normalized_links.append(normalized)\n",
        "            # Optimization: Use set lookup instead of checking queue\n",
        "            if normalized not in self._seen_paths:\n",
        "                self._seen_paths.add(normalized)\n",
        "                queue.append(normalized)\n",
        "        self.graph[path] = set(normalized_links)\n",
        "\n",
        "    def fetch_and_update(self, path: str) -> Tuple[PageState, bool]:\n",
        "        content = fetch_page(path)\n",
        "        parsed = parse_page(content, path)\n",
        "        self.page_visits += 1\n",
        "\n",
        "        page_id = parsed.get(\"page_id\", \"\")\n",
        "        node_id = parsed.get(\"node_id\", \"\")\n",
        "        history = parsed.get(\"history\", [])\n",
        "        outgoing = parsed.get(\"links\", [])\n",
        "        last_timestamp = parsed.get(\"last_updated\", \"\")\n",
        "\n",
        "        state = self.pages.get(path)\n",
        "        now = time.time()\n",
        "        is_new = state is None\n",
        "        changed = False\n",
        "\n",
        "        if state is None:\n",
        "            state = PageState(\n",
        "                page_id=page_id,\n",
        "                path=path,\n",
        "                last_node_id=node_id,\n",
        "                last_updated_at=last_timestamp,\n",
        "                history=history,\n",
        "                outgoing=outgoing,\n",
        "                last_fetched_ts=now,\n",
        "                last_changed_ts=now,\n",
        "            )\n",
        "            self.pages[path] = state\n",
        "            self._seen_paths.add(path)  # Mark as seen\n",
        "        else:\n",
        "            state.page_id = page_id or state.page_id\n",
        "            if state.differs_from(node_id, last_timestamp, history):\n",
        "                changed = True\n",
        "                state.last_node_id = node_id\n",
        "                state.last_updated_at = last_timestamp\n",
        "                state.history = history\n",
        "                state.last_changed_ts = now\n",
        "                state.updates_detected += 1\n",
        "                self.node_updates += 1\n",
        "            state.outgoing = outgoing\n",
        "            state.last_fetched_ts = now\n",
        "\n",
        "        self.graph[path] = set(outgoing)\n",
        "        # Optimization: Only log if needed (reduce memory)\n",
        "        if is_new or changed:\n",
        "            self.fetch_log.append({\n",
        "                \"path\": path,\n",
        "                \"timestamp\": now,\n",
        "                \"is_new\": is_new,\n",
        "                \"changed\": (changed and not is_new),\n",
        "            })\n",
        "        return state, (changed and not is_new)\n",
        "\n",
        "    def crawl(self, max_visits: int = 5000):\n",
        "        queue: deque[str] = deque([self.seed_path])\n",
        "        visited: Set[str] = set()\n",
        "        self._seen_paths.add(self.seed_path)\n",
        "        current_time = time.time()\n",
        "\n",
        "        while queue and self.page_visits < max_visits:\n",
        "            path = queue.popleft()\n",
        "            state = self.pages.get(path)\n",
        "\n",
        "            should_fetch = False\n",
        "            if state is None:\n",
        "                should_fetch = True\n",
        "            else:\n",
        "                # Optimization: Cache current time to avoid repeated calls\n",
        "                current_time = time.time()\n",
        "                if current_time - state.last_fetched_ts >= self.revisit_window:\n",
        "                    should_fetch = True\n",
        "\n",
        "            if not should_fetch:\n",
        "                continue\n",
        "\n",
        "            state, _ = self.fetch_and_update(path)\n",
        "            visited.add(path)\n",
        "            self.enqueue_links(path, state.outgoing, queue)\n",
        "\n",
        "        return visited\n",
        "\n",
        "    def refresh_due_pages(self, max_visits: int = 1000):\n",
        "        # Optimization: Calculate current time once\n",
        "        current_time = time.time()\n",
        "        # Optimization: Use list comprehension with filter for better performance\n",
        "        due_pages = [\n",
        "            path for path, state in self.pages.items()\n",
        "            if current_time - state.last_fetched_ts >= self.revisit_window\n",
        "        ]\n",
        "        # Optimization: Sort by staleness (oldest first) for better refresh order\n",
        "        due_pages.sort(key=lambda p: current_time - self.pages[p].last_fetched_ts, reverse=True)\n",
        "        \n",
        "        refreshed = []\n",
        "        updates_detected = 0\n",
        "        for path in due_pages[:max_visits]:  # Slice instead of break\n",
        "            _, updated = self.fetch_and_update(path)\n",
        "            refreshed.append(path)\n",
        "            if updated:\n",
        "                updates_detected += 1\n",
        "        return {\n",
        "            \"refreshed_paths\": refreshed,\n",
        "            \"updates_detected\": updates_detected,\n",
        "            \"fetches\": len(refreshed),\n",
        "        }\n",
        "\n",
        "    def build_pagerank_matrix(self):\n",
        "        # Optimization: Build node set more efficiently\n",
        "        all_nodes = set(self.graph.keys())\n",
        "        for dests in self.graph.values():\n",
        "            all_nodes.update(dests)\n",
        "        \n",
        "        nodes = sorted(all_nodes)\n",
        "        node_index = {node: idx for idx, node in enumerate(nodes)}\n",
        "        \n",
        "        # Optimization: Pre-allocate adjacency list\n",
        "        n = len(nodes)\n",
        "        adjacency = [[] for _ in range(n)]\n",
        "        \n",
        "        for src, dests in self.graph.items():\n",
        "            if src not in node_index:\n",
        "                continue\n",
        "            src_idx = node_index[src]\n",
        "            # Optimization: Use list comprehension with filter\n",
        "            adjacency[src_idx] = [node_index[d] for d in dests if d in node_index]\n",
        "        \n",
        "        return nodes, adjacency\n",
        "\n",
        "    def pagerank(self, damping: float = 0.85, max_iter: int = 100, tol: float = 1e-6):\n",
        "        nodes, adjacency = self.build_pagerank_matrix()\n",
        "        n = len(nodes)\n",
        "        if n == 0:\n",
        "            return {}\n",
        "        \n",
        "        pr = [1.0 / n] * n\n",
        "        teleport = (1.0 - damping) / n\n",
        "        sink_share = damping / n  # Pre-compute sink share\n",
        "\n",
        "        for iteration in range(max_iter):\n",
        "            new_pr = [teleport] * n\n",
        "            # Optimization: Pre-compute total sink contribution and add to all nodes once\n",
        "            sink_total = sum(pr[idx] for idx, neighbors in enumerate(adjacency) if not neighbors)\n",
        "            if sink_total > 0:\n",
        "                sink_contribution = sink_total * sink_share\n",
        "                for j in range(n):\n",
        "                    new_pr[j] += sink_contribution\n",
        "            \n",
        "            # Process non-sink nodes\n",
        "            for idx, neighbors in enumerate(adjacency):\n",
        "                if neighbors:  # Only process non-sinks\n",
        "                    share = damping * pr[idx] / len(neighbors)\n",
        "                    for dest_idx in neighbors:\n",
        "                        new_pr[dest_idx] += share\n",
        "            \n",
        "            # Optimization: Early convergence check with vectorized computation\n",
        "            delta = sum(abs(new_pr[i] - pr[i]) for i in range(n))\n",
        "            pr = new_pr\n",
        "            if delta < tol:\n",
        "                break\n",
        "        \n",
        "        return {nodes[i]: pr[i] for i in range(n)}\n",
        "\n",
        "    def summary(self) -> dict:\n",
        "        # Optimization: Single pass through pages\n",
        "        total_links = 0\n",
        "        unique_pages = len(self.pages)\n",
        "        for state in self.pages.values():\n",
        "            total_links += len(state.outgoing)\n",
        "        \n",
        "        return {\n",
        "            \"unique_pages\": unique_pages,\n",
        "            \"page_visits\": self.page_visits,\n",
        "            \"node_updates\": self.node_updates,\n",
        "            \"average_out_degree\": (total_links / unique_pages) if unique_pages else 0.0,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# crawler = EfficientCrawler(seed_path=\"/\", revisit_window=5.0)\n",
        "# visited = crawler.crawl(max_visits=2000)\n",
        "# visited_count = len(visited)\n",
        "# initial_summary = crawler.summary()\n",
        "# visited_count, crawler.page_visits, initial_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refresh Monitoring\n",
        "\n",
        "We periodically revisit pages (respecting the 5-second `revisit_window`) to track node-id churn while keeping the number of extra fetches low.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Submission\n",
        "\n",
        "The assignment requires submitting evaluations to `/evaluate` endpoint:\n",
        "- First evaluation within 15 seconds of first visit\n",
        "- Subsequent evaluations at least every 15 seconds\n",
        "- All evaluations within 60 seconds of first visit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ submit_evaluation function fixed! Now uses state.page_id instead of path.\n"
          ]
        }
      ],
      "source": [
        "# FIX: Replace submit_evaluation with corrected version\n",
        "# The original was using path instead of actual page_id from state\n",
        "\n",
        "def submit_evaluation(crawler: EfficientCrawler, pagerank_scores: dict) -> dict:\n",
        "    entries = []\n",
        "    for path, state in crawler.pages.items():\n",
        "        # Use the actual page_id from the page state, not the path\n",
        "        page_id = state.page_id\n",
        "        if not page_id or not page_id.strip():\n",
        "            # Fallback: extract from path if page_id is missing\n",
        "            page_id = path.lstrip(\"/\")\n",
        "            if not page_id:\n",
        "                continue  # Skip root path if no page_id available\n",
        "        \n",
        "        latest_node_id = state.last_node_id\n",
        "        if not latest_node_id or not latest_node_id.strip():\n",
        "            continue  # Skip entries without valid node_id\n",
        "        \n",
        "        score = pagerank_scores.get(path, 0.0)\n",
        "        entries.append({\n",
        "            \"page_id\": page_id,\n",
        "            \"latest_node_id\": latest_node_id,\n",
        "            \"score\": float(score),\n",
        "        })\n",
        "    \n",
        "    if not entries:\n",
        "        return {\"error\": \"No valid entries to submit (all entries missing page_id or node_id)\"}\n",
        "    \n",
        "    payload = {\"entries\": entries}\n",
        "    try:\n",
        "        response = session.post(\n",
        "            f\"{BASE_URL}/evaluate\",\n",
        "            json=payload,\n",
        "            timeout=5,\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        return result\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # Try to get error details from response\n",
        "        try:\n",
        "            if hasattr(e, 'response') and e.response is not None:\n",
        "                error_detail = e.response.json()\n",
        "                return {\"error\": str(e), \"detail\": error_detail}\n",
        "        except:\n",
        "            pass\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "print(\"✓ submit_evaluation function fixed! Now uses state.page_id instead of path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note on evaluation.bin\n",
        "\n",
        "The `evaluation.bin` file is created by the server (not the client) in the `/data` directory. According to the assignment instructions, it contains encrypted evaluation data for all evaluations submitted within the 60-second window. The file may be written:\n",
        "- After the 60-second window completes\n",
        "- When the server processes all submitted evaluations\n",
        "- The file location: `data/evaluation.bin` (if Docker volume is mounted correctly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠ evaluation.bin not found yet.\n",
            "  The server creates this file after processing evaluations.\n",
            "  It may appear after the 60-second window completes.\n",
            "  Expected location: f:\\IIITH\\COURSEWORK\\Monsoon_2025_SEM_IX\\IRE\\assignment 2\\data\\evaluation.bin\n",
            "\n",
            "  To check manually:\n",
            "    - Local: data\\evaluation.bin\n",
            "    - Docker: /data/evaluation.bin (inside container)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "evaluation_file = Path(\"data/evaluation.bin\")\n",
        "if evaluation_file.exists():\n",
        "    file_size = evaluation_file.stat().st_size\n",
        "    file_time = evaluation_file.stat().st_mtime\n",
        "    print(f\"✓ evaluation.bin found!\")\n",
        "    print(f\"  Size: {file_size} bytes\")\n",
        "    print(f\"  Last modified: {time.ctime(file_time)}\")\n",
        "else:\n",
        "    print(\"⚠ evaluation.bin not found yet.\")\n",
        "    print(\"  The server creates this file after processing evaluations.\")\n",
        "    print(\"  It may appear after the 60-second window completes.\")\n",
        "    print(f\"  Expected location: {evaluation_file.absolute()}\")\n",
        "    print(\"\\n  To check manually:\")\n",
        "    print(f\"    - Local: {evaluation_file}\")\n",
        "    print(f\"    - Docker: /data/evaluation.bin (inside container)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No evaluation results found. Run the evaluation cycle cell first.\n"
          ]
        }
      ],
      "source": [
        "if 'evaluation_results' in globals() and evaluation_results:\n",
        "    print(\"Evaluation Submission Summary:\")\n",
        "    print(f\"  Total evaluations submitted: {len(evaluation_results)}\")\n",
        "    \n",
        "    successful = [r for r in evaluation_results if 'error' not in r]\n",
        "    errors = [r for r in evaluation_results if 'error' in r]\n",
        "    \n",
        "    print(f\"  Successful: {len(successful)}\")\n",
        "    print(f\"  Errors: {len(errors)}\")\n",
        "    \n",
        "    if successful:\n",
        "        print(\"\\n  Last successful evaluation metrics:\")\n",
        "        last = successful[-1]\n",
        "        for key in ['mse', 'coverage', 'avg_staleness', 'visit_count', 'matched_entries']:\n",
        "            if key in last:\n",
        "                print(f\"    {key}: {last[key]}\")\n",
        "    \n",
        "    if errors:\n",
        "        print(\"\\n  Errors encountered:\")\n",
        "        for err in errors:\n",
        "            print(f\"    {err.get('error', 'Unknown error')}\")\n",
        "    \n",
        "    print(\"\\n  Note: evaluation.bin is created by the server after processing.\")\n",
        "    print(\"  If the file doesn't exist, the server may:\")\n",
        "    print(\"    - Write it after the 60-second window\")\n",
        "    print(\"    - Require all evaluations to be within timing constraints\")\n",
        "    print(\"    - Only write if evaluations are valid\")\n",
        "else:\n",
        "    print(\"No evaluation results found. Run the evaluation cycle cell first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First evaluation: {'avg_staleness': 14990.0, 'coverage': 0.9137931034482759, 'covered_nodes': 53, 'matched_entries': 54, 'mse': 3.1844068470508164e-06, 'total_nodes': 58, 'visit_count': 54, 'elapsed_seconds': 14.84568977355957}\n",
            "Evaluation at 29.65s: MSE=3.1844068470508164e-06, Coverage=0.9137931034482759, Staleness=29834.98148148148\n",
            "Evaluation at 44.49s: MSE=3.1844068470508164e-06, Coverage=0.9137931034482759, Staleness=44683.0\n",
            "Evaluation at 59.34s: MSE=3.1844068470508164e-06, Coverage=0.9137931034482759, Staleness=59529.0\n",
            "Evaluation at 74.19s: MSE=3.1844068470508164e-06, Coverage=0.9137931034482759, Staleness=74376.51851851853\n",
            "Evaluation at 89.04s: MSE=N/A, Coverage=N/A, Staleness=N/A\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avg_staleness</th>\n",
              "      <th>coverage</th>\n",
              "      <th>covered_nodes</th>\n",
              "      <th>matched_entries</th>\n",
              "      <th>mse</th>\n",
              "      <th>total_nodes</th>\n",
              "      <th>visit_count</th>\n",
              "      <th>elapsed_seconds</th>\n",
              "      <th>error</th>\n",
              "      <th>detail</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14990.000000</td>\n",
              "      <td>0.913793</td>\n",
              "      <td>53.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>58.0</td>\n",
              "      <td>54</td>\n",
              "      <td>14.845690</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29834.981481</td>\n",
              "      <td>0.913793</td>\n",
              "      <td>53.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>58.0</td>\n",
              "      <td>54</td>\n",
              "      <td>29.646465</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44683.000000</td>\n",
              "      <td>0.913793</td>\n",
              "      <td>53.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>58.0</td>\n",
              "      <td>54</td>\n",
              "      <td>44.492140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59529.000000</td>\n",
              "      <td>0.913793</td>\n",
              "      <td>53.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>58.0</td>\n",
              "      <td>54</td>\n",
              "      <td>59.339122</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>74376.518519</td>\n",
              "      <td>0.913793</td>\n",
              "      <td>53.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>58.0</td>\n",
              "      <td>54</td>\n",
              "      <td>74.186361</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>54</td>\n",
              "      <td>89.043301</td>\n",
              "      <td>400 Client Error: Bad Request for url: http://...</td>\n",
              "      <td>{'error': 'Evaluation window has ended. No fur...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   avg_staleness  coverage  covered_nodes  matched_entries       mse  \\\n",
              "0   14990.000000  0.913793           53.0             54.0  0.000003   \n",
              "1   29834.981481  0.913793           53.0             54.0  0.000003   \n",
              "2   44683.000000  0.913793           53.0             54.0  0.000003   \n",
              "3   59529.000000  0.913793           53.0             54.0  0.000003   \n",
              "4   74376.518519  0.913793           53.0             54.0  0.000003   \n",
              "5            NaN       NaN            NaN              NaN       NaN   \n",
              "\n",
              "   total_nodes  visit_count  elapsed_seconds  \\\n",
              "0         58.0           54        14.845690   \n",
              "1         58.0           54        29.646465   \n",
              "2         58.0           54        44.492140   \n",
              "3         58.0           54        59.339122   \n",
              "4         58.0           54        74.186361   \n",
              "5          NaN           54        89.043301   \n",
              "\n",
              "                                               error  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4                                                NaN   \n",
              "5  400 Client Error: Bad Request for url: http://...   \n",
              "\n",
              "                                              detail  \n",
              "0                                                NaN  \n",
              "1                                                NaN  \n",
              "2                                                NaN  \n",
              "3                                                NaN  \n",
              "4                                                NaN  \n",
              "5  {'error': 'Evaluation window has ended. No fur...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "crawler_eval = EfficientCrawler(seed_path=\"/\", revisit_window=5.0)\n",
        "visited_eval = crawler_eval.crawl(max_visits=10000)\n",
        "\n",
        "pagerank_scores_eval = crawler_eval.pagerank()\n",
        "\n",
        "first_visit_time = start_time\n",
        "evaluation_results = []\n",
        "\n",
        "elapsed = time.time() - first_visit_time\n",
        "if elapsed < 14.80:\n",
        "    time.sleep(14.80 - elapsed)\n",
        "\n",
        "result1 = submit_evaluation(crawler_eval, pagerank_scores_eval)\n",
        "result1[\"elapsed_seconds\"] = time.time() - first_visit_time\n",
        "result1[\"visit_count\"] = crawler_eval.page_visits\n",
        "evaluation_results.append(result1)\n",
        "print(f\"First evaluation: {result1}\")\n",
        "\n",
        "last_eval_time = time.time()\n",
        "while time.time() - first_visit_time <= 100.0:\n",
        "    time.sleep(14.80)\n",
        "    elapsed = time.time() - first_visit_time\n",
        "    if elapsed > 100.0:\n",
        "        break\n",
        "    \n",
        "    pagerank_scores_eval = crawler_eval.pagerank()\n",
        "    result = submit_evaluation(crawler_eval, pagerank_scores_eval)\n",
        "    result[\"elapsed_seconds\"] = elapsed\n",
        "    result[\"visit_count\"] = crawler_eval.page_visits\n",
        "    evaluation_results.append(result)\n",
        "    print(f\"Evaluation at {elapsed:.2f}s: MSE={result.get('mse', 'N/A')}, Coverage={result.get('coverage', 'N/A')}, Staleness={result.get('avg_staleness', 'N/A')}\")\n",
        "\n",
        "evaluation_summary = pd.DataFrame(evaluation_results)\n",
        "evaluation_summary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
