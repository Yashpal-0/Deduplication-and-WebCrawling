{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crawling Assignment Activity 2.2\n",
        "\n",
        "This notebook interacts with the local crawling assignment server (running at `http://localhost:3000`) to\n",
        "- discover the site graph with minimal page visits,\n",
        "- track `node_id` updates for each page, and\n",
        "- estimate PageRank scores over the discovered link structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Helper Functions\n",
        "\n",
        "The web server returns JSON responses. We use `requests` for HTTP and utilities for crawling and scoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install beautifulsoup4 --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import re\n",
        "from collections import deque, defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Set, Tuple\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"http://localhost:3000\"\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"CrawlerAssignmentBot/1.0\",\n",
        "    \"Accept\": \"text/html,application/json\",\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'path': '/',\n",
              " 'page_id': 'page_plate00v',\n",
              " 'node_id': 'nq0xg8jyhbba',\n",
              " 'last_updated': '2025-11-08 09:20:07 UTC',\n",
              " 'history': [],\n",
              " 'links': ['/page_00e9abwz',\n",
              "  '/page_48tad71n',\n",
              "  '/page_htotw746',\n",
              "  '/page_xjllli10',\n",
              "  '/page_yh6x0zj5']}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def fetch_page(path: str = \"/\") -> str:\n",
        "    url = BASE_URL.rstrip(\"/\") + path\n",
        "    response = session.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def parse_page(content: str, path: str) -> dict:\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "    page_id_text = soup.select_one(\".page-id\")\n",
        "    page_id = \"\"\n",
        "    if page_id_text:\n",
        "        page_id = page_id_text.get_text(strip=True).split(\":\")[-1].strip()\n",
        "\n",
        "    node_id_elem = soup.select_one(\".node-id b\")\n",
        "    node_id = node_id_elem.get_text(strip=True) if node_id_elem else \"\"\n",
        "\n",
        "    last_updated_elem = soup.select_one(\".last-updated\")\n",
        "    last_updated = \"\"\n",
        "    if last_updated_elem:\n",
        "        last_updated = last_updated_elem.get_text(strip=True)\n",
        "        if \":\" in last_updated:\n",
        "            last_updated = last_updated.split(\":\", 1)[-1].strip()\n",
        "\n",
        "    history_entries: List[dict] = []\n",
        "    history_container = soup.select_one(\"details\")\n",
        "    if history_container:\n",
        "        for entry in history_container.select(\"div\"):\n",
        "            text = entry.get_text(strip=True)\n",
        "            text = text.strip(\"\\u0007 \\n\\r\\t\")\n",
        "            match = re.match(r\"([A-Za-z0-9]+)\\s*\\(([^)]+)\\)\", text)\n",
        "            if match:\n",
        "                history_entries.append({\n",
        "                    \"node_id\": match.group(1),\n",
        "                    \"timestamp\": match.group(2),\n",
        "                })\n",
        "\n",
        "    outgoing_links: List[str] = []\n",
        "    for link in soup.select(\"a.file-link\"):\n",
        "        href = link.get(\"href\")\n",
        "        if href and href.startswith(\"/page_\"):\n",
        "            outgoing_links.append(href)\n",
        "    outgoing_links = sorted(set(outgoing_links))\n",
        "\n",
        "    return {\n",
        "        \"path\": path,\n",
        "        \"page_id\": page_id,\n",
        "        \"node_id\": node_id,\n",
        "        \"last_updated\": last_updated,\n",
        "        \"history\": history_entries,\n",
        "        \"links\": outgoing_links,\n",
        "    }\n",
        "\n",
        "\n",
        "root_html = fetch_page(\"/\")\n",
        "root_parsed = parse_page(root_html, \"/\")\n",
        "root_parsed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PageState:\n",
        "    page_id: str\n",
        "    path: str\n",
        "    last_node_id: str\n",
        "    last_updated_at: str\n",
        "    history: List[dict] = field(default_factory=list)\n",
        "    outgoing: List[str] = field(default_factory=list)\n",
        "    last_fetched_ts: float = field(default_factory=time.time)\n",
        "    last_changed_ts: float = field(default_factory=time.time)\n",
        "    updates_detected: int = 0\n",
        "\n",
        "    def differs_from(self, node_id: str, last_updated: str, history: List[dict]) -> bool:\n",
        "        if node_id != self.last_node_id:\n",
        "            return True\n",
        "        if last_updated and last_updated != self.last_updated_at:\n",
        "            return True\n",
        "        if len(history) != len(self.history):\n",
        "            return True\n",
        "        if history and self.history:\n",
        "            return history[-1] != self.history[-1]\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientCrawler:\n",
        "    def __init__(self, seed_path: str = \"/\", revisit_window: float = 120.0):\n",
        "        self.seed_path = seed_path\n",
        "        self.revisit_window = revisit_window\n",
        "        self.pages: Dict[str, PageState] = {}\n",
        "        self.graph: Dict[str, Set[str]] = defaultdict(set)\n",
        "        self.page_visits = 0\n",
        "        self.node_updates = 0\n",
        "        self.fetch_log: List[dict] = []\n",
        "\n",
        "    def normalize_path(self, path: str) -> str:\n",
        "        if path.startswith(\"http://\") or path.startswith(\"https://\"):\n",
        "            if path.startswith(BASE_URL):\n",
        "                path = path[len(BASE_URL):]\n",
        "            else:\n",
        "                return path  # external link\n",
        "        if not path.startswith(\"/\"):\n",
        "            path = \"/\" + path\n",
        "        return path\n",
        "\n",
        "    def enqueue_links(self, path: str, links: List[str], queue: deque):\n",
        "        normalized_links = []\n",
        "        for link in links:\n",
        "            normalized = self.normalize_path(link)\n",
        "            if not normalized.startswith(\"/\"):\n",
        "                continue  # skip external\n",
        "            normalized_links.append(normalized)\n",
        "            if normalized not in self.pages and normalized not in queue:\n",
        "                queue.append(normalized)\n",
        "        self.graph[path] = set(normalized_links)\n",
        "\n",
        "    def fetch_and_update(self, path: str) -> Tuple[PageState, bool]:\n",
        "        content = fetch_page(path)\n",
        "        parsed = parse_page(content, path)\n",
        "        self.page_visits += 1\n",
        "\n",
        "        page_id = parsed.get(\"page_id\", \"\")\n",
        "        node_id = parsed.get(\"node_id\", \"\")\n",
        "        history = parsed.get(\"history\", [])\n",
        "        outgoing = parsed.get(\"links\", [])\n",
        "        last_timestamp = parsed.get(\"last_updated\", \"\")\n",
        "\n",
        "        state = self.pages.get(path)\n",
        "        now = time.time()\n",
        "        is_new = state is None\n",
        "        changed = False\n",
        "\n",
        "        if state is None:\n",
        "            state = PageState(\n",
        "                page_id=page_id,\n",
        "                path=path,\n",
        "                last_node_id=node_id,\n",
        "                last_updated_at=last_timestamp,\n",
        "                history=history,\n",
        "                outgoing=outgoing,\n",
        "                last_fetched_ts=now,\n",
        "                last_changed_ts=now,\n",
        "            )\n",
        "            self.pages[path] = state\n",
        "        else:\n",
        "            state.page_id = page_id or state.page_id\n",
        "            if state.differs_from(node_id, last_timestamp, history):\n",
        "                changed = True\n",
        "                state.last_node_id = node_id\n",
        "                state.last_updated_at = last_timestamp\n",
        "                state.history = history\n",
        "                state.last_changed_ts = now\n",
        "                state.updates_detected += 1\n",
        "                self.node_updates += 1\n",
        "            state.outgoing = outgoing\n",
        "            state.last_fetched_ts = now\n",
        "\n",
        "        self.graph[path] = set(outgoing)\n",
        "        self.fetch_log.append({\n",
        "            \"path\": path,\n",
        "            \"timestamp\": now,\n",
        "            \"is_new\": is_new,\n",
        "            \"changed\": (changed and not is_new),\n",
        "        })\n",
        "        return state, (changed and not is_new)\n",
        "\n",
        "    def crawl(self, max_visits: int = 5000):\n",
        "        queue: deque[str] = deque([self.seed_path])\n",
        "        visited: Set[str] = set()\n",
        "\n",
        "        while queue and self.page_visits < max_visits:\n",
        "            path = queue.popleft()\n",
        "            state = self.pages.get(path)\n",
        "\n",
        "            should_fetch = False\n",
        "            if state is None:\n",
        "                should_fetch = True\n",
        "            else:\n",
        "                if time.time() - state.last_fetched_ts >= self.revisit_window:\n",
        "                    should_fetch = True\n",
        "\n",
        "            if not should_fetch:\n",
        "                continue\n",
        "\n",
        "            state, _ = self.fetch_and_update(path)\n",
        "            visited.add(path)\n",
        "            self.enqueue_links(path, state.outgoing, queue)\n",
        "\n",
        "        return visited\n",
        "\n",
        "    def refresh_due_pages(self, max_visits: int = 1000):\n",
        "        due_pages = sorted(\n",
        "            (\n",
        "                (time.time() - state.last_fetched_ts, path)\n",
        "                for path, state in self.pages.items()\n",
        "            ),\n",
        "            reverse=True,\n",
        "        )\n",
        "        refreshed = []\n",
        "        extra_visits = 0\n",
        "        updates_detected = 0\n",
        "        for _, path in due_pages:\n",
        "            if extra_visits >= max_visits:\n",
        "                break\n",
        "            state = self.pages[path]\n",
        "            if time.time() - state.last_fetched_ts < self.revisit_window:\n",
        "                continue\n",
        "            _, updated = self.fetch_and_update(path)\n",
        "            refreshed.append(path)\n",
        "            if updated:\n",
        "                updates_detected += 1\n",
        "            extra_visits += 1\n",
        "        return {\n",
        "            \"refreshed_paths\": refreshed,\n",
        "            \"updates_detected\": updates_detected,\n",
        "            \"fetches\": extra_visits,\n",
        "        }\n",
        "\n",
        "    def build_pagerank_matrix(self):\n",
        "        nodes = list(self.graph.keys() | {link for links in self.graph.values() for link in links})\n",
        "        nodes.sort()\n",
        "        node_index = {node: idx for idx, node in enumerate(nodes)}\n",
        "        adjacency = [[] for _ in nodes]\n",
        "        for src, dests in self.graph.items():\n",
        "            if src not in node_index:\n",
        "                continue\n",
        "            src_idx = node_index[src]\n",
        "            adjacency[src_idx] = [node_index[d] for d in dests if d in node_index]\n",
        "        return nodes, adjacency\n",
        "\n",
        "    def pagerank(self, damping: float = 0.85, max_iter: int = 100, tol: float = 1e-6):\n",
        "        nodes, adjacency = self.build_pagerank_matrix()\n",
        "        n = len(nodes)\n",
        "        if n == 0:\n",
        "            return {}\n",
        "        pr = [1.0 / n] * n\n",
        "        teleport = (1.0 - damping) / n\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            new_pr = [teleport] * n\n",
        "            for idx, neighbors in enumerate(adjacency):\n",
        "                if not neighbors:\n",
        "                    share = damping * pr[idx] / n\n",
        "                    for j in range(n):\n",
        "                        new_pr[j] += share\n",
        "                else:\n",
        "                    share = damping * pr[idx] / len(neighbors)\n",
        "                    for dest_idx in neighbors:\n",
        "                        new_pr[dest_idx] += share\n",
        "            delta = sum(abs(new_pr[i] - pr[i]) for i in range(n))\n",
        "            pr = new_pr\n",
        "            if delta < tol:\n",
        "                break\n",
        "        return {nodes[i]: pr[i] for i in range(n)}\n",
        "\n",
        "    def summary(self) -> dict:\n",
        "        total_links = sum(len(state.outgoing) for state in self.pages.values())\n",
        "        unique_pages = len(self.pages)\n",
        "        return {\n",
        "            \"unique_pages\": unique_pages,\n",
        "            \"page_visits\": self.page_visits,\n",
        "            \"node_updates\": self.node_updates,\n",
        "            \"average_out_degree\": (total_links / unique_pages) if unique_pages else 0.0,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15,\n",
              " 15,\n",
              " {'unique_pages': 15,\n",
              "  'page_visits': 15,\n",
              "  'node_updates': 0,\n",
              "  'average_out_degree': 3.533333333333333})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "crawler = EfficientCrawler(seed_path=\"/\", revisit_window=5.0)\n",
        "visited = crawler.crawl(max_visits=2000)\n",
        "visited_count = len(visited)\n",
        "initial_summary = crawler.summary()\n",
        "visited_count, crawler.page_visits, initial_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pagerank_scores = crawler.pagerank()\n",
        "len(pagerank_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('/page_xjllli10', 0.11898420117468086),\n",
              " ('/page_6glvhfj2', 0.11283934870979398),\n",
              " ('/page_1v6o8hdh', 0.10739699512938083),\n",
              " ('/page_plate00v', 0.09811409131131249),\n",
              " ('/page_00e9abwz', 0.08459863267602709),\n",
              " ('/page_t5ymb8dk', 0.0765973192936817),\n",
              " ('/page_qn6ex9sa', 0.07177323669335728),\n",
              " ('/page_yh6x0zj5', 0.06396623840496077),\n",
              " ('/page_48tad71n', 0.04836706379395495),\n",
              " ('/page_htotw746', 0.04836706379395495)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted_scores = sorted(pagerank_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
        "sorted_scores[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>page_id</th>\n",
              "      <th>last_node_id</th>\n",
              "      <th>last_update</th>\n",
              "      <th>links</th>\n",
              "      <th>history_len</th>\n",
              "      <th>last_fetched_ts</th>\n",
              "      <th>last_changed_ts</th>\n",
              "      <th>updates_detected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/</td>\n",
              "      <td>page_plate00v</td>\n",
              "      <td>nq0xg8jyhbba</td>\n",
              "      <td>2025-11-08 09:20:07 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/page_00e9abwz</td>\n",
              "      <td>page_00e9abwz</td>\n",
              "      <td>im3j17j8duu4</td>\n",
              "      <td>2025-11-08 09:19:22 UTC</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>/page_1v6o8hdh</td>\n",
              "      <td>page_1v6o8hdh</td>\n",
              "      <td>5oyzg8d7f1ps</td>\n",
              "      <td>2025-11-08 09:19:30 UTC</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/page_48tad71n</td>\n",
              "      <td>page_48tad71n</td>\n",
              "      <td>9dilt11ljax3</td>\n",
              "      <td>2025-11-08 09:20:08 UTC</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>/page_6glvhfj2</td>\n",
              "      <td>page_6glvhfj2</td>\n",
              "      <td>t6ppm9u5dgeo</td>\n",
              "      <td>2025-11-08 09:20:07 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>1.762594e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              path        page_id  last_node_id              last_update  \\\n",
              "0                /  page_plate00v  nq0xg8jyhbba  2025-11-08 09:20:07 UTC   \n",
              "1   /page_00e9abwz  page_00e9abwz  im3j17j8duu4  2025-11-08 09:19:22 UTC   \n",
              "6   /page_1v6o8hdh  page_1v6o8hdh  5oyzg8d7f1ps  2025-11-08 09:19:30 UTC   \n",
              "2   /page_48tad71n  page_48tad71n  9dilt11ljax3  2025-11-08 09:20:08 UTC   \n",
              "11  /page_6glvhfj2  page_6glvhfj2  t6ppm9u5dgeo  2025-11-08 09:20:07 UTC   \n",
              "\n",
              "    links  history_len  last_fetched_ts  last_changed_ts  updates_detected  \n",
              "0       5            0     1.762594e+09     1.762594e+09                 0  \n",
              "1       2            0     1.762594e+09     1.762594e+09                 0  \n",
              "6       4            0     1.762594e+09     1.762594e+09                 0  \n",
              "2       3            0     1.762594e+09     1.762594e+09                 0  \n",
              "11      5            0     1.762594e+09     1.762594e+09                 0  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "page_summary = pd.DataFrame([\n",
        "    {\n",
        "        \"path\": path,\n",
        "        \"page_id\": state.page_id,\n",
        "        \"last_node_id\": state.last_node_id,\n",
        "        \"last_update\": state.last_updated_at,\n",
        "        \"links\": len(state.outgoing),\n",
        "        \"history_len\": len(state.history),\n",
        "        \"last_fetched_ts\": state.last_fetched_ts,\n",
        "        \"last_changed_ts\": state.last_changed_ts,\n",
        "        \"updates_detected\": state.updates_detected,\n",
        "    }\n",
        "    for path, state in crawler.pages.items()\n",
        "]).sort_values(\"path\")\n",
        "page_summary.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['updates_detected'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpagerank\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpagerank\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates_detected\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlinks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yashp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yashp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yashp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"['updates_detected'] not in index\""
          ]
        }
      ],
      "source": [
        "results_df.sort_values(\"pagerank\", ascending=False).head(10)[[\"path\", \"pagerank\", \"updates_detected\", \"links\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>page_id</th>\n",
              "      <th>last_node_id</th>\n",
              "      <th>last_update</th>\n",
              "      <th>links</th>\n",
              "      <th>history_len</th>\n",
              "      <th>last_fetched_ts</th>\n",
              "      <th>pagerank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/</td>\n",
              "      <td>page_plate00v</td>\n",
              "      <td>cs06ec0pnxkm</td>\n",
              "      <td>2025-11-08 09:07:49 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762593e+09</td>\n",
              "      <td>0.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/page_00e9abwz</td>\n",
              "      <td>page_00e9abwz</td>\n",
              "      <td>sk1x7ubd9zee</td>\n",
              "      <td>2025-11-08 09:07:38 UTC</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762593e+09</td>\n",
              "      <td>0.084599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/page_1v6o8hdh</td>\n",
              "      <td>page_1v6o8hdh</td>\n",
              "      <td>vumshv53cx75</td>\n",
              "      <td>2025-11-08 09:08:01 UTC</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762593e+09</td>\n",
              "      <td>0.107397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/page_48tad71n</td>\n",
              "      <td>page_48tad71n</td>\n",
              "      <td>8kzqs8yj7dah</td>\n",
              "      <td>2025-11-08 09:08:06 UTC</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762593e+09</td>\n",
              "      <td>0.048367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/page_6glvhfj2</td>\n",
              "      <td>page_6glvhfj2</td>\n",
              "      <td>vmuazmx3buwr</td>\n",
              "      <td>2025-11-08 09:08:04 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.762593e+09</td>\n",
              "      <td>0.112839</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             path        page_id  last_node_id              last_update  \\\n",
              "0               /  page_plate00v  cs06ec0pnxkm  2025-11-08 09:07:49 UTC   \n",
              "1  /page_00e9abwz  page_00e9abwz  sk1x7ubd9zee  2025-11-08 09:07:38 UTC   \n",
              "2  /page_1v6o8hdh  page_1v6o8hdh  vumshv53cx75  2025-11-08 09:08:01 UTC   \n",
              "3  /page_48tad71n  page_48tad71n  8kzqs8yj7dah  2025-11-08 09:08:06 UTC   \n",
              "4  /page_6glvhfj2  page_6glvhfj2  vmuazmx3buwr  2025-11-08 09:08:04 UTC   \n",
              "\n",
              "   links  history_len  last_fetched_ts  pagerank  \n",
              "0      5            0     1.762593e+09  0.010000  \n",
              "1      2            0     1.762593e+09  0.084599  \n",
              "2      4            0     1.762593e+09  0.107397  \n",
              "3      3            0     1.762593e+09  0.048367  \n",
              "4      5            0     1.762593e+09  0.112839  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pagerank_df = pd.DataFrame(sorted_scores, columns=[\"path\", \"pagerank\"])\n",
        "results_df = page_summary.merge(pagerank_df, on=\"path\", how=\"left\")\n",
        "results_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refresh Monitoring\n",
        "\n",
        "We periodically revisit pages (respecting the 5-second `revisit_window`) to track node-id churn while keeping the number of extra fetches low.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "monitor_log = []\n",
        "for cycle in range(3):\n",
        "    time.sleep(6)\n",
        "    refresh_stats = crawler.refresh_due_pages(max_visits=20)\n",
        "    summary = crawler.summary()\n",
        "    monitor_log.append({\n",
        "        \"cycle\": cycle + 1,\n",
        "        \"refreshed\": len(refresh_stats[\"refreshed_paths\"]),\n",
        "        \"updates_detected\": refresh_stats[\"updates_detected\"],\n",
        "        \"fetches\": refresh_stats[\"fetches\"],\n",
        "        \"total_page_visits\": summary[\"page_visits\"],\n",
        "        \"total_updates\": summary[\"node_updates\"],\n",
        "    })\n",
        "\n",
        "monitor_df = pd.DataFrame(monitor_log)\n",
        "monitor_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "updated_pages = [\n",
        "    {\n",
        "        \"path\": path,\n",
        "        \"page_id\": state.page_id,\n",
        "        \"last_node_id\": state.last_node_id,\n",
        "        \"updates_detected\": state.updates_detected,\n",
        "        \"last_change_ts\": state.last_changed_ts,\n",
        "    }\n",
        "    for path, state in crawler.pages.items()\n",
        "    if state.updates_detected > 0\n",
        "]\n",
        "\n",
        "updated_pages_df = pd.DataFrame(updated_pages)\n",
        "updated_pages_df.sort_values(\"updates_detected\", ascending=False) if not updated_pages_df.empty else \"No node-id updates observed during monitoring window.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_summary = crawler.summary()\n",
        "final_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def pages_due_for_refresh(crawler: EfficientCrawler, horizon: float = 120.0):\n",
        "    now = time.time()\n",
        "    due = []\n",
        "    for path, state in crawler.pages.items():\n",
        "        if now - state.last_fetched_ts >= horizon:\n",
        "            due.append(path)\n",
        "    return due\n",
        "\n",
        "pending_refresh = pages_due_for_refresh(crawler, horizon=10.0)\n",
        "{\"due_count\": len(pending_refresh), \"sample\": pending_refresh[:5]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Updated Discussion\n",
        "\n",
        "- **HTML Parsing**: The crawler now treats every response as HTML and extracts structured data via BeautifulSoup, so it keeps working even if the server never exposes a JSON API.\n",
        "- **Visit Minimisation**: The initial crawl touched 15 pages with 15 fetches (`initial_summary`), and each refresh cycle caps re-fetches at 20 while respecting the 5-second `revisit_window`.\n",
        "- **Node ID Tracking**: `monitor_df` shows how many revisits were required per cycle and how many true node-id changes were captured; `updated_pages_df` lists the specific pages where changes occurred.\n",
        "- **Ranking Output**: PageRank results are merged back into `results_df`, letting us correlate high-importance pages with their churn frequency (`updates_detected`).\n",
        "- **Efficiency Snapshot**: `final_summary` records total fetches and updates after the monitoring loop, demonstrating that node-id tracking adds only a small number of extra page visits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repro Tips\n",
        "\n",
        "- Run the notebook top-to-bottom while the Docker container is active; the 5-second `revisit_window` means the monitoring loop pauses briefly between refreshes.\n",
        "- If the server rotates node IDs less frequently, increase the number of monitoring cycles or the `time.sleep` delay to capture more changes.\n",
        "- To audit efficiency, inspect `monitor_df`, `updated_pages_df`, and `final_summary`â€”they quantify extra fetches and observed node-id churn.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
