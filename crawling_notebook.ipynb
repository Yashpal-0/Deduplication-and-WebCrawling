{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crawling Assignment Activity 2.2\n",
        "\n",
        "This notebook interacts with the local crawling assignment server (running at `http://localhost:3000`) to\n",
        "- discover the site graph with minimal page visits,\n",
        "- track `node_id` updates for each page, and\n",
        "- estimate PageRank scores over the discovered link structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Helper Functions\n",
        "\n",
        "The web server returns JSON responses. We use `requests` for HTTP and utilities for crawling and scoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.14.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import re\n",
        "from collections import deque, defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Set, Tuple\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"http://localhost:3000\"\n",
        "\n",
        "# ULTRA-OPTIMIZED: Connection pooling for parallel requests\n",
        "session = requests.Session()\n",
        "adapter = requests.adapters.HTTPAdapter(\n",
        "    pool_connections=100,  # Max connection pools\n",
        "    pool_maxsize=100,      # Max connections per pool\n",
        "    max_retries=0,         # No retries for speed\n",
        ")\n",
        "session.mount('http://', adapter)\n",
        "session.mount('https://', adapter)\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"CrawlerAssignmentBot/1.2\",\n",
        "    \"Accept\": \"text/html,application/json\",\n",
        "    \"Connection\": \"keep-alive\",  # Reuse connections\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'path': '/',\n",
              " 'page_id': 'page_rdwv1o91',\n",
              " 'node_id': 'm3y9q8rv4m01',\n",
              " 'last_updated': '2025-12-05 11:58:05 UTC',\n",
              " 'history': [],\n",
              " 'links': ['/page_1and89kh',\n",
              "  '/page_l07vms0e',\n",
              "  '/page_o41nvqbo',\n",
              "  '/page_o8uri2ox']}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def fetch_page(path: str = \"/\") -> str:\n",
        "    \"\"\"Optimized fetch with shorter timeout for parallel requests\"\"\"\n",
        "    url = BASE_URL.rstrip(\"/\") + path\n",
        "    response = session.get(url, timeout=3)  # Shorter timeout for faster parallel execution\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def parse_page(content: str, path: str) -> dict:\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "    page_id_text = soup.select_one(\".page-id\")\n",
        "    page_id = \"\"\n",
        "    if page_id_text:\n",
        "        page_id = page_id_text.get_text(strip=True).split(\":\")[-1].strip()\n",
        "\n",
        "    node_id_elem = soup.select_one(\".node-id b\")\n",
        "    node_id = node_id_elem.get_text(strip=True) if node_id_elem else \"\"\n",
        "\n",
        "    last_updated_elem = soup.select_one(\".last-updated\")\n",
        "    last_updated = \"\"\n",
        "    if last_updated_elem:\n",
        "        last_updated = last_updated_elem.get_text(strip=True)\n",
        "        if \":\" in last_updated:\n",
        "            last_updated = last_updated.split(\":\", 1)[-1].strip()\n",
        "\n",
        "    history_entries: List[dict] = []\n",
        "    history_container = soup.select_one(\"details\")\n",
        "    if history_container:\n",
        "        for entry in history_container.select(\"div\"):\n",
        "            text = entry.get_text(strip=True)\n",
        "            text = text.strip(\"\\u0007 \\n\\r\\t\")\n",
        "            match = re.match(r\"([A-Za-z0-9]+)\\s*\\(([^)]+)\\)\", text)\n",
        "            if match:\n",
        "                history_entries.append({\n",
        "                    \"node_id\": match.group(1),\n",
        "                    \"timestamp\": match.group(2),\n",
        "                })\n",
        "\n",
        "    outgoing_links: List[str] = []\n",
        "    for link in soup.select(\"a.file-link\"):\n",
        "        href = link.get(\"href\")\n",
        "        if href and href.startswith(\"/page_\"):\n",
        "            outgoing_links.append(href)\n",
        "    outgoing_links = sorted(set(outgoing_links))\n",
        "\n",
        "    return {\n",
        "        \"path\": path,\n",
        "        \"page_id\": page_id,\n",
        "        \"node_id\": node_id,\n",
        "        \"last_updated\": last_updated,\n",
        "        \"history\": history_entries,\n",
        "        \"links\": outgoing_links,\n",
        "    }\n",
        "\n",
        "\n",
        "root_html = fetch_page(\"/\")\n",
        "root_parsed = parse_page(root_html, \"/\")\n",
        "root_parsed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PageState:\n",
        "    page_id: str\n",
        "    path: str\n",
        "    last_node_id: str\n",
        "    last_updated_at: str\n",
        "    history: List[dict] = field(default_factory=list)\n",
        "    outgoing: List[str] = field(default_factory=list)\n",
        "    last_fetched_ts: float = field(default_factory=time.time)\n",
        "    last_changed_ts: float = field(default_factory=time.time)\n",
        "    updates_detected: int = 0\n",
        "\n",
        "    def differs_from(self, node_id: str, last_updated: str, history: List[dict]) -> bool:\n",
        "        if node_id != self.last_node_id:\n",
        "            return True\n",
        "        if last_updated and last_updated != self.last_updated_at:\n",
        "            return True\n",
        "        if len(history) != len(self.history):\n",
        "            return True\n",
        "        if history and self.history:\n",
        "            return history[-1] != self.history[-1]\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Ultra-optimized parallel refresh methods added for minimal staleness!\n"
          ]
        }
      ],
      "source": [
        "# ULTRA-OPTIMIZED: Add parallel refresh methods to EfficientCrawler for minimal staleness\n",
        "def _fetch_and_update_single(self, path: str) -> Tuple[str, PageState, bool]:\n",
        "    \"\"\"Helper for parallel fetching - returns (path, state, updated)\"\"\"\n",
        "    try:\n",
        "        state, updated = self.fetch_and_update(path)\n",
        "        return path, state, updated\n",
        "    except Exception as e:\n",
        "        # Return existing state if fetch fails\n",
        "        state = self.pages.get(path)\n",
        "        return path, state, False\n",
        "\n",
        "def refresh_all_pages_aggressively(self, max_workers: int = 50) -> dict:\n",
        "    \"\"\"\n",
        "    ULTRA-OPTIMIZED: Parallel refresh of ALL pages to minimize staleness.\n",
        "    Uses ThreadPoolExecutor to fetch all pages simultaneously.\n",
        "    This is CRITICAL for minimizing staleness in competition!\n",
        "    \"\"\"\n",
        "    if not self.pages:\n",
        "        return {\"refreshed_paths\": [], \"updates_detected\": 0, \"fetches\": 0}\n",
        "    \n",
        "    # Get all pages, sorted by staleness (oldest first) for priority\n",
        "    current_time = time.time()\n",
        "    all_pages = sorted(\n",
        "        self.pages.keys(),\n",
        "        key=lambda p: current_time - self.pages[p].last_fetched_ts,\n",
        "        reverse=True\n",
        "    )\n",
        "    \n",
        "    # CRITICAL: Use parallel fetching to minimize total refresh time\n",
        "    # This dramatically reduces staleness by refreshing all pages simultaneously\n",
        "    refreshed = []\n",
        "    updates_detected = 0\n",
        "    lock = Lock()\n",
        "    \n",
        "    # Parallel fetch with ThreadPoolExecutor\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all fetch tasks\n",
        "        future_to_path = {\n",
        "            executor.submit(self._fetch_and_update_single, path): path\n",
        "            for path in all_pages\n",
        "        }\n",
        "        \n",
        "        # Process results as they complete\n",
        "        for future in as_completed(future_to_path):\n",
        "            try:\n",
        "                path, state, updated = future.result(timeout=5)\n",
        "                if state is not None:\n",
        "                    refreshed.append(path)\n",
        "                    if updated:\n",
        "                        with lock:\n",
        "                            updates_detected += 1\n",
        "            except Exception as e:\n",
        "                # Skip failed fetches\n",
        "                pass\n",
        "    \n",
        "    return {\n",
        "        \"refreshed_paths\": refreshed,\n",
        "        \"updates_detected\": updates_detected,\n",
        "        \"fetches\": len(refreshed),\n",
        "    }\n",
        "\n",
        "# Bind methods to class\n",
        "\n",
        "print(\"‚úì Ultra-optimized parallel refresh methods added for minimal staleness!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientCrawler:\n",
        "    \"\"\"\n",
        "    Optimized crawler with the following efficiency improvements:\n",
        "    1. Path normalization caching to avoid repeated string operations\n",
        "    2. Seen paths tracking (set) for O(1) duplicate detection vs O(n) queue checks\n",
        "    3. Time caching to reduce system calls\n",
        "    4. Selective logging (only new/changed pages) to reduce memory\n",
        "    5. Optimized refresh: single pass filter + slice instead of break\n",
        "    6. Efficient graph building: set operations instead of unions\n",
        "    7. PageRank optimization: pre-compute sink contributions, separate sink/non-sink processing\n",
        "    8. Single-pass summary calculation\n",
        "    \"\"\"\n",
        "    def __init__(self, seed_path: str = \"/\", revisit_window: float = 120.0):\n",
        "        self.seed_path = seed_path\n",
        "        self.revisit_window = revisit_window\n",
        "        self.pages: Dict[str, PageState] = {}\n",
        "        self.graph: Dict[str, Set[str]] = defaultdict(set)\n",
        "        self.page_visits = 0\n",
        "        self.node_updates = 0\n",
        "        self.fetch_log: List[dict] = []\n",
        "        # Optimization: Track seen paths to avoid duplicate queue entries\n",
        "        self._seen_paths: Set[str] = set()\n",
        "        # Optimization: Cache normalized paths\n",
        "        self._path_cache: Dict[str, str] = {}\n",
        "\n",
        "    def normalize_path(self, path: str) -> str:\n",
        "        # Use cache to avoid repeated normalization\n",
        "        if path in self._path_cache:\n",
        "            return self._path_cache[path]\n",
        "        \n",
        "        original_path = path\n",
        "        if path.startswith(\"http://\") or path.startswith(\"https://\"):\n",
        "            if path.startswith(BASE_URL):\n",
        "                path = path[len(BASE_URL):]\n",
        "            else:\n",
        "                self._path_cache[original_path] = path\n",
        "                return path  # external link\n",
        "        if not path.startswith(\"/\"):\n",
        "            path = \"/\" + path\n",
        "        \n",
        "        self._path_cache[original_path] = path\n",
        "        return path\n",
        "\n",
        "    def enqueue_links(self, path: str, links: List[str], queue: deque):\n",
        "        normalized_links = []\n",
        "        for link in links:\n",
        "            normalized = self.normalize_path(link)\n",
        "            if not normalized.startswith(\"/\"):\n",
        "                continue  # skip external\n",
        "            normalized_links.append(normalized)\n",
        "            # Optimization: Use set lookup instead of checking queue\n",
        "            if normalized not in self._seen_paths:\n",
        "                self._seen_paths.add(normalized)\n",
        "                queue.append(normalized)\n",
        "        self.graph[path] = set(normalized_links)\n",
        "\n",
        "    def fetch_and_update(self, path: str) -> Tuple[PageState, bool]:\n",
        "        content = fetch_page(path)\n",
        "        parsed = parse_page(content, path)\n",
        "        self.page_visits += 1\n",
        "\n",
        "        page_id = parsed.get(\"page_id\", \"\")\n",
        "        node_id = parsed.get(\"node_id\", \"\")\n",
        "        history = parsed.get(\"history\", [])\n",
        "        outgoing = parsed.get(\"links\", [])\n",
        "        last_timestamp = parsed.get(\"last_updated\", \"\")\n",
        "\n",
        "        state = self.pages.get(path)\n",
        "        now = time.time()\n",
        "        is_new = state is None\n",
        "        changed = False\n",
        "\n",
        "        if state is None:\n",
        "            state = PageState(\n",
        "                page_id=page_id,\n",
        "                path=path,\n",
        "                last_node_id=node_id,\n",
        "                last_updated_at=last_timestamp,\n",
        "                history=history,\n",
        "                outgoing=outgoing,\n",
        "                last_fetched_ts=now,\n",
        "                last_changed_ts=now,\n",
        "            )\n",
        "            self.pages[path] = state\n",
        "            self._seen_paths.add(path)  # Mark as seen\n",
        "        else:\n",
        "            state.page_id = page_id or state.page_id\n",
        "            if state.differs_from(node_id, last_timestamp, history):\n",
        "                changed = True\n",
        "                state.last_node_id = node_id\n",
        "                state.last_updated_at = last_timestamp\n",
        "                state.history = history\n",
        "                state.last_changed_ts = now\n",
        "                state.updates_detected += 1\n",
        "                self.node_updates += 1\n",
        "            state.outgoing = outgoing\n",
        "            state.last_fetched_ts = now\n",
        "\n",
        "        self.graph[path] = set(outgoing)\n",
        "        # Optimization: Only log if needed (reduce memory)\n",
        "        if is_new or changed:\n",
        "            self.fetch_log.append({\n",
        "                \"path\": path,\n",
        "                \"timestamp\": now,\n",
        "                \"is_new\": is_new,\n",
        "                \"changed\": (changed and not is_new),\n",
        "            })\n",
        "        return state, (changed and not is_new)\n",
        "\n",
        "    def crawl(self, max_visits: int = 5000):\n",
        "        queue: deque[str] = deque([self.seed_path])\n",
        "        visited: Set[str] = set()\n",
        "        self._seen_paths.add(self.seed_path)\n",
        "        current_time = time.time()\n",
        "\n",
        "        while queue and self.page_visits < max_visits:\n",
        "            path = queue.popleft()\n",
        "            state = self.pages.get(path)\n",
        "\n",
        "            should_fetch = False\n",
        "            if state is None:\n",
        "                should_fetch = True\n",
        "            else:\n",
        "                # Optimization: Cache current time to avoid repeated calls\n",
        "                current_time = time.time()\n",
        "                if current_time - state.last_fetched_ts >= self.revisit_window:\n",
        "                    should_fetch = True\n",
        "\n",
        "            if not should_fetch:\n",
        "                continue\n",
        "\n",
        "            state, _ = self.fetch_and_update(path)\n",
        "            visited.add(path)\n",
        "            self.enqueue_links(path, state.outgoing, queue)\n",
        "\n",
        "        return visited\n",
        "\n",
        "    def refresh_due_pages(self, max_visits: int = 1000):\n",
        "        # Optimization: Calculate current time once\n",
        "        current_time = time.time()\n",
        "        # Optimization: Use list comprehension with filter for better performance\n",
        "        due_pages = [\n",
        "            path for path, state in self.pages.items()\n",
        "            if current_time - state.last_fetched_ts >= self.revisit_window\n",
        "        ]\n",
        "        # Optimization: Sort by staleness (oldest first) for better refresh order\n",
        "        due_pages.sort(key=lambda p: current_time - self.pages[p].last_fetched_ts, reverse=True)\n",
        "        \n",
        "        refreshed = []\n",
        "        updates_detected = 0\n",
        "        for path in due_pages[:max_visits]:  # Slice instead of break\n",
        "            _, updated = self.fetch_and_update(path)\n",
        "            refreshed.append(path)\n",
        "            if updated:\n",
        "                updates_detected += 1\n",
        "        return {\n",
        "            \"refreshed_paths\": refreshed,\n",
        "            \"updates_detected\": updates_detected,\n",
        "            \"fetches\": len(refreshed),\n",
        "        }\n",
        "\n",
        "    def build_pagerank_matrix(self):\n",
        "        # Optimization: Build node set more efficiently\n",
        "        all_nodes = set(self.graph.keys())\n",
        "        for dests in self.graph.values():\n",
        "            all_nodes.update(dests)\n",
        "        \n",
        "        nodes = sorted(all_nodes)\n",
        "        node_index = {node: idx for idx, node in enumerate(nodes)}\n",
        "        \n",
        "        # Optimization: Pre-allocate adjacency list\n",
        "        n = len(nodes)\n",
        "        adjacency = [[] for _ in range(n)]\n",
        "        \n",
        "        for src, dests in self.graph.items():\n",
        "            if src not in node_index:\n",
        "                continue\n",
        "            src_idx = node_index[src]\n",
        "            # Optimization: Use list comprehension with filter\n",
        "            adjacency[src_idx] = [node_index[d] for d in dests if d in node_index]\n",
        "        \n",
        "        return nodes, adjacency\n",
        "\n",
        "    def pagerank(self, damping: float = 0.85, max_iter: int = 100, tol: float = 1e-6):\n",
        "        nodes, adjacency = self.build_pagerank_matrix()\n",
        "        n = len(nodes)\n",
        "        if n == 0:\n",
        "            return {}\n",
        "        \n",
        "        pr = [1.0 / n] * n\n",
        "        teleport = (1.0 - damping) / n\n",
        "        sink_share = damping / n  # Pre-compute sink share\n",
        "\n",
        "        for iteration in range(max_iter):\n",
        "            new_pr = [teleport] * n\n",
        "            # Optimization: Pre-compute total sink contribution and add to all nodes once\n",
        "            sink_total = sum(pr[idx] for idx, neighbors in enumerate(adjacency) if not neighbors)\n",
        "            if sink_total > 0:\n",
        "                sink_contribution = sink_total * sink_share\n",
        "                for j in range(n):\n",
        "                    new_pr[j] += sink_contribution\n",
        "            \n",
        "            # Process non-sink nodes\n",
        "            for idx, neighbors in enumerate(adjacency):\n",
        "                if neighbors:  # Only process non-sinks\n",
        "                    share = damping * pr[idx] / len(neighbors)\n",
        "                    for dest_idx in neighbors:\n",
        "                        new_pr[dest_idx] += share\n",
        "            \n",
        "            # Optimization: Early convergence check with vectorized computation\n",
        "            delta = sum(abs(new_pr[i] - pr[i]) for i in range(n))\n",
        "            pr = new_pr\n",
        "            if delta < tol:\n",
        "                break\n",
        "        \n",
        "        return {nodes[i]: pr[i] for i in range(n)}\n",
        "\n",
        "    def summary(self) -> dict:\n",
        "        # Optimization: Single pass through pages\n",
        "        total_links = 0\n",
        "        unique_pages = len(self.pages)\n",
        "        for state in self.pages.values():\n",
        "            total_links += len(state.outgoing)\n",
        "        \n",
        "        return {\n",
        "            \"unique_pages\": unique_pages,\n",
        "            \"page_visits\": self.page_visits,\n",
        "            \"node_updates\": self.node_updates,\n",
        "            \"average_out_degree\": (total_links / unique_pages) if unique_pages else 0.0,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# crawler = EfficientCrawler(seed_path=\"/\", revisit_window=5.0)\n",
        "# visited = crawler.crawl(max_visits=2000)\n",
        "# visited_count = len(visited)\n",
        "# initial_summary = crawler.summary()\n",
        "# visited_count, crawler.page_visits, initial_summary\n",
        "EfficientCrawler._fetch_and_update_single = _fetch_and_update_single\n",
        "EfficientCrawler.refresh_all_pages_aggressively = refresh_all_pages_aggressively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refresh Monitoring\n",
        "\n",
        "We periodically revisit pages (respecting the 5-second `revisit_window`) to track node-id churn while keeping the number of extra fetches low.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Submission\n",
        "\n",
        "The assignment requires submitting evaluations to `/evaluate` endpoint:\n",
        "- First evaluation within 15 seconds of first visit\n",
        "- Subsequent evaluations at least every 15 seconds\n",
        "- All evaluations within 60 seconds of first visit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì submit_evaluation function fixed! Now uses state.page_id instead of path.\n"
          ]
        }
      ],
      "source": [
        "# FIX: Replace submit_evaluation with corrected version\n",
        "# The original was using path instead of actual page_id from state\n",
        "\n",
        "def submit_evaluation(crawler: EfficientCrawler, pagerank_scores: dict) -> dict:\n",
        "    entries = []\n",
        "    for path, state in crawler.pages.items():\n",
        "        # Use the actual page_id from the page state, not the path\n",
        "        page_id = state.page_id\n",
        "        if not page_id or not page_id.strip():\n",
        "            # Fallback: extract from path if page_id is missing\n",
        "            page_id = path.lstrip(\"/\")\n",
        "            if not page_id:\n",
        "                continue  # Skip root path if no page_id available\n",
        "        \n",
        "        latest_node_id = state.last_node_id\n",
        "        if not latest_node_id or not latest_node_id.strip():\n",
        "            continue  # Skip entries without valid node_id\n",
        "        \n",
        "        score = pagerank_scores.get(path, 0.0)\n",
        "        entries.append({\n",
        "            \"page_id\": page_id,\n",
        "            \"latest_node_id\": latest_node_id,\n",
        "            \"score\": float(score),\n",
        "        })\n",
        "    \n",
        "    if not entries:\n",
        "        return {\"error\": \"No valid entries to submit (all entries missing page_id or node_id)\"}\n",
        "    \n",
        "    payload = {\"entries\": entries}\n",
        "    try:\n",
        "        response = session.post(\n",
        "            f\"{BASE_URL}/evaluate\",\n",
        "            json=payload,\n",
        "            timeout=5,\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        return result\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # Try to get error details from response\n",
        "        try:\n",
        "            if hasattr(e, 'response') and e.response is not None:\n",
        "                error_detail = e.response.json()\n",
        "                return {\"error\": str(e), \"detail\": error_detail}\n",
        "        except:\n",
        "            pass\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "print(\"‚úì submit_evaluation function fixed! Now uses state.page_id instead of path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note on evaluation.bin\n",
        "\n",
        "The `evaluation.bin` file is created by the server (not the client) in the `/data` directory. According to the assignment instructions, it contains encrypted evaluation data for all evaluations submitted within the 60-second window. The file may be written:\n",
        "- After the 60-second window completes\n",
        "- When the server processes all submitted evaluations\n",
        "- The file location: `data/evaluation.bin` (if Docker volume is mounted correctly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì evaluation.bin found!\n",
            "  Size: 39904 bytes\n",
            "  Last modified: Wed Nov 19 00:31:23 2025\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "evaluation_file = Path(\"data/evaluation.bin\")\n",
        "if evaluation_file.exists():\n",
        "    file_size = evaluation_file.stat().st_size\n",
        "    file_time = evaluation_file.stat().st_mtime\n",
        "    print(f\"‚úì evaluation.bin found!\")\n",
        "    print(f\"  Size: {file_size} bytes\")\n",
        "    print(f\"  Last modified: {time.ctime(file_time)}\")\n",
        "else:\n",
        "    print(\"‚ö† evaluation.bin not found yet.\")\n",
        "    print(\"  The server creates this file after processing evaluations.\")\n",
        "    print(\"  It may appear after the 60-second window completes.\")\n",
        "    print(f\"  Expected location: {evaluation_file.absolute()}\")\n",
        "    print(\"\\n  To check manually:\")\n",
        "    print(f\"    - Local: {evaluation_file}\")\n",
        "    print(f\"    - Docker: /data/evaluation.bin (inside container)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Run Checklist\n",
        "\n",
        "**‚ö†Ô∏è CRITICAL: Before running the evaluation cycle, ensure:**\n",
        "\n",
        "1. ‚úÖ Docker container is **stopped and restarted** to reset server state\n",
        "   ```bash\n",
        "   docker stop <container-id>\n",
        "   docker run --rm -p 3000:3000 -v $(pwd)/data:/data --tmpfs /tmp:rw,noexec,nosuid --cap-drop ALL --security-opt no-new-privileges --pids-limit 128 --memory 256m crawling_assignment:1.2\n",
        "   ```\n",
        "\n",
        "2. ‚úÖ Server is running and accessible at `http://localhost:3000`\n",
        "\n",
        "3. ‚úÖ All cells above have been executed (imports, functions, class definitions)\n",
        "\n",
        "4. ‚úÖ You're ready to run the evaluation cycle (it will take ~60 seconds)\n",
        "\n",
        "**The optimized evaluation cycle will:**\n",
        "- Discover all pages efficiently\n",
        "- Refresh ALL pages in parallel before each evaluation\n",
        "- Submit 4-5 evaluations within the 60-second window\n",
        "- Minimize staleness to < 5000ms (target for competition)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Submission Summary:\n",
            "  Total evaluations submitted: 3\n",
            "  Successful: 3\n",
            "  Errors: 0\n",
            "\n",
            "  Last successful evaluation metrics:\n",
            "    mse: 9.251565123424178e-06\n",
            "    coverage: 0.9833333333333333\n",
            "    avg_staleness: 7216.916666666667\n",
            "    visit_count: 240\n",
            "    matched_entries: 60\n",
            "\n",
            "  Note: evaluation.bin is created by the server after processing.\n",
            "  If the file doesn't exist, the server may:\n",
            "    - Write it after the 60-second window\n",
            "    - Require all evaluations to be within timing constraints\n",
            "    - Only write if evaluations are valid\n"
          ]
        }
      ],
      "source": [
        "if 'evaluation_results' in globals() and evaluation_results:\n",
        "    print(\"Evaluation Submission Summary:\")\n",
        "    print(f\"  Total evaluations submitted: {len(evaluation_results)}\")\n",
        "    \n",
        "    successful = [r for r in evaluation_results if 'error' not in r]\n",
        "    errors = [r for r in evaluation_results if 'error' in r]\n",
        "    \n",
        "    print(f\"  Successful: {len(successful)}\")\n",
        "    print(f\"  Errors: {len(errors)}\")\n",
        "    \n",
        "    if successful:\n",
        "        print(\"\\n  Last successful evaluation metrics:\")\n",
        "        last = successful[-1]\n",
        "        for key in ['mse', 'coverage', 'avg_staleness', 'visit_count', 'matched_entries']:\n",
        "            if key in last:\n",
        "                print(f\"    {key}: {last[key]}\")\n",
        "    \n",
        "    if errors:\n",
        "        print(\"\\n  Errors encountered:\")\n",
        "        for err in errors:\n",
        "            print(f\"    {err.get('error', 'Unknown error')}\")\n",
        "    \n",
        "    print(\"\\n  Note: evaluation.bin is created by the server after processing.\")\n",
        "    print(\"  If the file doesn't exist, the server may:\")\n",
        "    print(\"    - Write it after the 60-second window\")\n",
        "    print(\"    - Require all evaluations to be within timing constraints\")\n",
        "    print(\"    - Only write if evaluations are valid\")\n",
        "else:\n",
        "    print(\"No evaluation results found. Run the evaluation cycle cell first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting ULTRA-OPTIMIZED evaluation cycle for minimal staleness...\n",
            "‚ö†Ô∏è  IMPORTANT: Restart Docker container before running to reset server state!\n",
            "\n",
            "üì° Crawling website to discover all pages...\n",
            "   ‚úì Discovered 60 pages with 60 visits\n",
            "üìä Calculating PageRank...\n",
            "   ‚úì PageRank calculated for 60 pages\n",
            "\n",
            "üîÑ FIRST EVALUATION: Refreshing all pages in parallel to minimize staleness...\n",
            "   ‚úì Refreshed 60 pages in 0.339s (53 node IDs updated)\n",
            "   üì§ First evaluation submitted at 15.39s\n",
            "      Staleness: 6962.483333333334 ms, Coverage: 0.983, MSE: 9.25e-06\n",
            "\n",
            "üîÑ SUBSEQUENT EVALUATIONS:\n",
            "\n",
            "üîÑ Evaluation #2: Refreshing all pages in parallel...\n",
            "   ‚úì Refreshed 60 pages in 0.284s (51 node IDs updated)\n",
            "   üì§ Evaluation #2 at 30.19s: Staleness=7541.183333333333 ms, Coverage=0.983, MSE=9.25e-06\n",
            "\n",
            "üîÑ Evaluation #3: Refreshing all pages in parallel...\n",
            "   ‚úì Refreshed 60 pages in 0.273s (55 node IDs updated)\n",
            "   üì§ Evaluation #3 at 45.32s: Staleness=6041.383333333333 ms, Coverage=0.983, MSE=9.25e-06\n",
            "\n",
            "‚úÖ Evaluation cycle complete! Submitted 3 evaluations\n",
            "   Total visits: 240\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avg_staleness</th>\n",
              "      <th>coverage</th>\n",
              "      <th>covered_nodes</th>\n",
              "      <th>matched_entries</th>\n",
              "      <th>mse</th>\n",
              "      <th>total_nodes</th>\n",
              "      <th>visit_count</th>\n",
              "      <th>elapsed_seconds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6962.483333</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>60</td>\n",
              "      <td>120</td>\n",
              "      <td>15.387426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7541.183333</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>60</td>\n",
              "      <td>180</td>\n",
              "      <td>30.187862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6041.383333</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>60</td>\n",
              "      <td>240</td>\n",
              "      <td>45.318926</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   avg_staleness  coverage  covered_nodes  matched_entries       mse  \\\n",
              "0    6962.483333  0.983333             59               60  0.000009   \n",
              "1    7541.183333  0.983333             59               60  0.000009   \n",
              "2    6041.383333  0.983333             59               60  0.000009   \n",
              "\n",
              "   total_nodes  visit_count  elapsed_seconds  \n",
              "0           60          120        15.387426  \n",
              "1           60          180        30.187862  \n",
              "2           60          240        45.318926  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ULTRA-OPTIMIZED EVALUATION CYCLE: Minimize staleness for competition\n",
        "# Strategy: Refresh ALL pages in parallel right before each evaluation submission\n",
        "\n",
        "# Ensure refresh_all_pages_aggressively method exists\n",
        "if not hasattr(EfficientCrawler, 'refresh_all_pages_aggressively'):\n",
        "    def _fetch_and_update_single(self, path: str) -> Tuple[str, PageState, bool]:\n",
        "        \"\"\"Helper for parallel fetching - returns (path, state, updated)\"\"\"\n",
        "        try:\n",
        "            state, updated = self.fetch_and_update(path)\n",
        "            return path, state, updated\n",
        "        except Exception as e:\n",
        "            state = self.pages.get(path)\n",
        "            return path, state, False\n",
        "    \n",
        "    def refresh_all_pages_aggressively(self, max_workers: int = 50) -> dict:\n",
        "        \"\"\"Aggressively refresh ALL pages to get the freshest node IDs.\"\"\"\n",
        "        if not self.pages:\n",
        "            return {\"refreshed_paths\": [], \"updates_detected\": 0, \"fetches\": 0}\n",
        "        current_time = time.time()\n",
        "        all_pages = sorted(\n",
        "            self.pages.keys(),\n",
        "            key=lambda p: current_time - self.pages[p].last_fetched_ts,\n",
        "            reverse=True\n",
        "        )\n",
        "        refreshed = []\n",
        "        updates_detected = 0\n",
        "        lock = Lock()\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            future_to_path = {\n",
        "                executor.submit(self._fetch_and_update_single, path): path\n",
        "                for path in all_pages\n",
        "            }\n",
        "            for future in as_completed(future_to_path):\n",
        "                try:\n",
        "                    path, state, updated = future.result(timeout=5)\n",
        "                    if state is not None:\n",
        "                        refreshed.append(path)\n",
        "                        if updated:\n",
        "                            with lock:\n",
        "                                updates_detected += 1\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "        return {\n",
        "            \"refreshed_paths\": refreshed,\n",
        "            \"updates_detected\": updates_detected,\n",
        "            \"fetches\": len(refreshed),\n",
        "        }\n",
        "    EfficientCrawler._fetch_and_update_single = _fetch_and_update_single\n",
        "    EfficientCrawler.refresh_all_pages_aggressively = refresh_all_pages_aggressively\n",
        "\n",
        "# Ensure submit_evaluation function exists\n",
        "if 'submit_evaluation' not in globals():\n",
        "    def submit_evaluation(crawler: EfficientCrawler, pagerank_scores: dict) -> dict:\n",
        "        entries = []\n",
        "        for path, state in crawler.pages.items():\n",
        "            page_id = state.page_id\n",
        "            if not page_id or not page_id.strip():\n",
        "                page_id = path.lstrip(\"/\")\n",
        "                if not page_id:\n",
        "                    continue\n",
        "            latest_node_id = state.last_node_id\n",
        "            if not latest_node_id or not latest_node_id.strip():\n",
        "                continue\n",
        "            score = pagerank_scores.get(path, 0.0)\n",
        "            entries.append({\n",
        "                \"page_id\": page_id,\n",
        "                \"latest_node_id\": latest_node_id,\n",
        "                \"score\": float(score),\n",
        "            })\n",
        "        if not entries:\n",
        "            return {\"error\": \"No valid entries to submit\"}\n",
        "        payload = {\"entries\": entries}\n",
        "        try:\n",
        "            response = session.post(\n",
        "                f\"{BASE_URL}/evaluate\",\n",
        "                json=payload,\n",
        "                timeout=5,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            try:\n",
        "                if hasattr(e, 'response') and e.response is not None:\n",
        "                    error_detail = e.response.json()\n",
        "                    return {\"error\": str(e), \"detail\": error_detail}\n",
        "            except:\n",
        "                pass\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "print(\"üöÄ Starting ULTRA-OPTIMIZED evaluation cycle for minimal staleness...\")\n",
        "print(\"‚ö†Ô∏è  IMPORTANT: Restart Docker container before running to reset server state!\")\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Step 1: Initial crawl to discover all pages\n",
        "crawler_eval = EfficientCrawler(seed_path=\"/\", revisit_window=5.0)\n",
        "print(\"üì° Crawling website to discover all pages...\")\n",
        "visited_eval = crawler_eval.crawl(max_visits=10000)\n",
        "print(f\"   ‚úì Discovered {len(visited_eval)} pages with {crawler_eval.page_visits} visits\")\n",
        "\n",
        "# Step 2: Calculate initial PageRank\n",
        "print(\"üìä Calculating PageRank...\")\n",
        "pagerank_scores_eval = crawler_eval.pagerank()\n",
        "print(f\"   ‚úì PageRank calculated for {len(pagerank_scores_eval)} pages\")\n",
        "\n",
        "first_visit_time = start_time\n",
        "evaluation_results = []\n",
        "\n",
        "# Step 3: Wait until ~14.8 seconds (just before 15s deadline)\n",
        "elapsed = time.time() - first_visit_time\n",
        "if elapsed < 14.80:\n",
        "    time.sleep(14.80 - elapsed)\n",
        "\n",
        "# Step 4: CRITICAL - Refresh ALL pages in parallel right before first evaluation\n",
        "print(\"\\nüîÑ FIRST EVALUATION: Refreshing all pages in parallel to minimize staleness...\")\n",
        "refresh_start = time.time()\n",
        "refresh_stats = crawler_eval.refresh_all_pages_aggressively(max_workers=50)\n",
        "refresh_time = time.time() - refresh_start\n",
        "print(f\"   ‚úì Refreshed {refresh_stats['fetches']} pages in {refresh_time:.3f}s ({refresh_stats['updates_detected']} node IDs updated)\")\n",
        "\n",
        "# Recalculate PageRank after refresh (in case graph changed)\n",
        "pagerank_scores_eval = crawler_eval.pagerank()\n",
        "\n",
        "# Submit first evaluation immediately after refresh\n",
        "result1 = submit_evaluation(crawler_eval, pagerank_scores_eval)\n",
        "result1[\"elapsed_seconds\"] = time.time() - first_visit_time\n",
        "result1[\"visit_count\"] = crawler_eval.page_visits\n",
        "evaluation_results.append(result1)\n",
        "print(f\"   üì§ First evaluation submitted at {result1['elapsed_seconds']:.2f}s\")\n",
        "if 'error' not in result1:\n",
        "    print(f\"      Staleness: {result1.get('avg_staleness', 'N/A')} ms, Coverage: {result1.get('coverage', 'N/A'):.3f}, MSE: {result1.get('mse', 'N/A'):.2e}\")\n",
        "else:\n",
        "    print(f\"      ‚ùå Error: {result1.get('error', 'Unknown')}\")\n",
        "\n",
        "# Step 5: Subsequent evaluations every ~14.8 seconds\n",
        "print(\"\\nüîÑ SUBSEQUENT EVALUATIONS:\")\n",
        "last_eval_time = time.time()\n",
        "eval_count = 1\n",
        "\n",
        "while time.time() - first_visit_time <= 60.0:  # Stay within 60s window\n",
        "    # Wait until ~14.8 seconds after last evaluation\n",
        "    elapsed_since_last = time.time() - last_eval_time\n",
        "    if elapsed_since_last < 14.80:\n",
        "        time.sleep(14.80 - elapsed_since_last)\n",
        "    \n",
        "    elapsed = time.time() - first_visit_time\n",
        "    if elapsed > 60.0:\n",
        "        break\n",
        "    \n",
        "    eval_count += 1\n",
        "    \n",
        "    # CRITICAL: Refresh ALL pages in parallel right before each evaluation\n",
        "    print(f\"\\nüîÑ Evaluation #{eval_count}: Refreshing all pages in parallel...\")\n",
        "    refresh_start = time.time()\n",
        "    refresh_stats = crawler_eval.refresh_all_pages_aggressively(max_workers=50)\n",
        "    refresh_time = time.time() - refresh_start\n",
        "    print(f\"   ‚úì Refreshed {refresh_stats['fetches']} pages in {refresh_time:.3f}s ({refresh_stats['updates_detected']} node IDs updated)\")\n",
        "    \n",
        "    # Recalculate PageRank after refresh\n",
        "    pagerank_scores_eval = crawler_eval.pagerank()\n",
        "    \n",
        "    # Submit evaluation immediately after refresh\n",
        "    result = submit_evaluation(crawler_eval, pagerank_scores_eval)\n",
        "    result[\"elapsed_seconds\"] = elapsed\n",
        "    result[\"visit_count\"] = crawler_eval.page_visits\n",
        "    evaluation_results.append(result)\n",
        "    \n",
        "    if 'error' not in result:\n",
        "        print(f\"   üì§ Evaluation #{eval_count} at {elapsed:.2f}s: Staleness={result.get('avg_staleness', 'N/A')} ms, Coverage={result.get('coverage', 'N/A'):.3f}, MSE={result.get('mse', 'N/A'):.2e}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Evaluation #{eval_count} at {elapsed:.2f}s: Error - {result.get('error', 'Unknown')}\")\n",
        "    \n",
        "    last_eval_time = time.time()\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation cycle complete! Submitted {len(evaluation_results)} evaluations\")\n",
        "print(f\"   Total visits: {crawler_eval.page_visits}\")\n",
        "\n",
        "evaluation_summary = pd.DataFrame(evaluation_results)\n",
        "evaluation_summary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
