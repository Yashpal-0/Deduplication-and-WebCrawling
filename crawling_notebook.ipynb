{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crawling Assignment Activity 2.2\n",
        "\n",
        "This notebook interacts with the local crawling assignment server (running at `http://localhost:3000`) to\n",
        "- discover the site graph with minimal page visits,\n",
        "- track `node_id` updates for each page, and\n",
        "- estimate PageRank scores over the discovered link structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Helper Functions\n",
        "\n",
        "The web server returns JSON responses. We use `requests` for HTTP and utilities for crawling and scoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install beautifulsoup4 --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import re\n",
        "from collections import deque, defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Set, Tuple\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"http://localhost:3000\"\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"CrawlerAssignmentBot/1.0\",\n",
        "    \"Accept\": \"text/html,application/json\",\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'path': '/',\n",
              " 'page_id': 'page_pjm3eihj',\n",
              " 'node_id': '9hfmc9s6jcm7',\n",
              " 'last_updated': '2025-11-15 11:20:09 UTC',\n",
              " 'history': [],\n",
              " 'links': ['/page_13q7cjnj',\n",
              "  '/page_8u5qc2wf',\n",
              "  '/page_w6ju6bgi',\n",
              "  '/page_yn8o949j',\n",
              "  '/page_zvdv4uqs']}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def fetch_page(path: str = \"/\") -> str:\n",
        "    url = BASE_URL.rstrip(\"/\") + path\n",
        "    response = session.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def parse_page(content: str, path: str) -> dict:\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "    page_id_text = soup.select_one(\".page-id\")\n",
        "    page_id = \"\"\n",
        "    if page_id_text:\n",
        "        page_id = page_id_text.get_text(strip=True).split(\":\")[-1].strip()\n",
        "\n",
        "    node_id_elem = soup.select_one(\".node-id b\")\n",
        "    node_id = node_id_elem.get_text(strip=True) if node_id_elem else \"\"\n",
        "\n",
        "    last_updated_elem = soup.select_one(\".last-updated\")\n",
        "    last_updated = \"\"\n",
        "    if last_updated_elem:\n",
        "        last_updated = last_updated_elem.get_text(strip=True)\n",
        "        if \":\" in last_updated:\n",
        "            last_updated = last_updated.split(\":\", 1)[-1].strip()\n",
        "\n",
        "    history_entries: List[dict] = []\n",
        "    history_container = soup.select_one(\"details\")\n",
        "    if history_container:\n",
        "        for entry in history_container.select(\"div\"):\n",
        "            text = entry.get_text(strip=True)\n",
        "            text = text.strip(\"\\u0007 \\n\\r\\t\")\n",
        "            match = re.match(r\"([A-Za-z0-9]+)\\s*\\(([^)]+)\\)\", text)\n",
        "            if match:\n",
        "                history_entries.append({\n",
        "                    \"node_id\": match.group(1),\n",
        "                    \"timestamp\": match.group(2),\n",
        "                })\n",
        "\n",
        "    outgoing_links: List[str] = []\n",
        "    for link in soup.select(\"a.file-link\"):\n",
        "        href = link.get(\"href\")\n",
        "        if href and href.startswith(\"/page_\"):\n",
        "            outgoing_links.append(href)\n",
        "    outgoing_links = sorted(set(outgoing_links))\n",
        "\n",
        "    return {\n",
        "        \"path\": path,\n",
        "        \"page_id\": page_id,\n",
        "        \"node_id\": node_id,\n",
        "        \"last_updated\": last_updated,\n",
        "        \"history\": history_entries,\n",
        "        \"links\": outgoing_links,\n",
        "    }\n",
        "\n",
        "\n",
        "root_html = fetch_page(\"/\")\n",
        "root_parsed = parse_page(root_html, \"/\")\n",
        "root_parsed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PageState:\n",
        "    page_id: str\n",
        "    path: str\n",
        "    last_node_id: str\n",
        "    last_updated_at: str\n",
        "    history: List[dict] = field(default_factory=list)\n",
        "    outgoing: List[str] = field(default_factory=list)\n",
        "    last_fetched_ts: float = field(default_factory=time.time)\n",
        "    last_changed_ts: float = field(default_factory=time.time)\n",
        "    updates_detected: int = 0\n",
        "\n",
        "    def differs_from(self, node_id: str, last_updated: str, history: List[dict]) -> bool:\n",
        "        if node_id != self.last_node_id:\n",
        "            return True\n",
        "        if last_updated and last_updated != self.last_updated_at:\n",
        "            return True\n",
        "        if len(history) != len(self.history):\n",
        "            return True\n",
        "        if history and self.history:\n",
        "            return history[-1] != self.history[-1]\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientCrawler:\n",
        "    def __init__(self, seed_path: str = \"/\", revisit_window: float = 120.0):\n",
        "        self.seed_path = seed_path\n",
        "        self.revisit_window = revisit_window\n",
        "        self.pages: Dict[str, PageState] = {}\n",
        "        self.graph: Dict[str, Set[str]] = defaultdict(set)\n",
        "        self.page_visits = 0\n",
        "        self.node_updates = 0\n",
        "        self.fetch_log: List[dict] = []\n",
        "\n",
        "    def normalize_path(self, path: str) -> str:\n",
        "        if path.startswith(\"http://\") or path.startswith(\"https://\"):\n",
        "            if path.startswith(BASE_URL):\n",
        "                path = path[len(BASE_URL):]\n",
        "            else:\n",
        "                return path  # external link\n",
        "        if not path.startswith(\"/\"):\n",
        "            path = \"/\" + path\n",
        "        return path\n",
        "\n",
        "    def enqueue_links(self, path: str, links: List[str], queue: deque):\n",
        "        normalized_links = []\n",
        "        for link in links:\n",
        "            normalized = self.normalize_path(link)\n",
        "            if not normalized.startswith(\"/\"):\n",
        "                continue  # skip external\n",
        "            normalized_links.append(normalized)\n",
        "            if normalized not in self.pages and normalized not in queue:\n",
        "                queue.append(normalized)\n",
        "        self.graph[path] = set(normalized_links)\n",
        "\n",
        "    def fetch_and_update(self, path: str) -> Tuple[PageState, bool]:\n",
        "        content = fetch_page(path)\n",
        "        parsed = parse_page(content, path)\n",
        "        self.page_visits += 1\n",
        "\n",
        "        page_id = parsed.get(\"page_id\", \"\")\n",
        "        node_id = parsed.get(\"node_id\", \"\")\n",
        "        history = parsed.get(\"history\", [])\n",
        "        outgoing = parsed.get(\"links\", [])\n",
        "        last_timestamp = parsed.get(\"last_updated\", \"\")\n",
        "\n",
        "        state = self.pages.get(path)\n",
        "        now = time.time()\n",
        "        is_new = state is None\n",
        "        changed = False\n",
        "\n",
        "        if state is None:\n",
        "            state = PageState(\n",
        "                page_id=page_id,\n",
        "                path=path,\n",
        "                last_node_id=node_id,\n",
        "                last_updated_at=last_timestamp,\n",
        "                history=history,\n",
        "                outgoing=outgoing,\n",
        "                last_fetched_ts=now,\n",
        "                last_changed_ts=now,\n",
        "            )\n",
        "            self.pages[path] = state\n",
        "        else:\n",
        "            state.page_id = page_id or state.page_id\n",
        "            if state.differs_from(node_id, last_timestamp, history):\n",
        "                changed = True\n",
        "                state.last_node_id = node_id\n",
        "                state.last_updated_at = last_timestamp\n",
        "                state.history = history\n",
        "                state.last_changed_ts = now\n",
        "                state.updates_detected += 1\n",
        "                self.node_updates += 1\n",
        "            state.outgoing = outgoing\n",
        "            state.last_fetched_ts = now\n",
        "\n",
        "        self.graph[path] = set(outgoing)\n",
        "        self.fetch_log.append({\n",
        "            \"path\": path,\n",
        "            \"timestamp\": now,\n",
        "            \"is_new\": is_new,\n",
        "            \"changed\": (changed and not is_new),\n",
        "        })\n",
        "        return state, (changed and not is_new)\n",
        "\n",
        "    def crawl(self, max_visits: int = 5000):\n",
        "        queue: deque[str] = deque([self.seed_path])\n",
        "        visited: Set[str] = set()\n",
        "\n",
        "        while queue and self.page_visits < max_visits:\n",
        "            path = queue.popleft()\n",
        "            state = self.pages.get(path)\n",
        "\n",
        "            should_fetch = False\n",
        "            if state is None:\n",
        "                should_fetch = True\n",
        "            else:\n",
        "                if time.time() - state.last_fetched_ts >= self.revisit_window:\n",
        "                    should_fetch = True\n",
        "\n",
        "            if not should_fetch:\n",
        "                continue\n",
        "\n",
        "            state, _ = self.fetch_and_update(path)\n",
        "            visited.add(path)\n",
        "            self.enqueue_links(path, state.outgoing, queue)\n",
        "\n",
        "        return visited\n",
        "\n",
        "    def refresh_due_pages(self, max_visits: int = 1000):\n",
        "        due_pages = sorted(\n",
        "            (\n",
        "                (time.time() - state.last_fetched_ts, path)\n",
        "                for path, state in self.pages.items()\n",
        "            ),\n",
        "            reverse=True,\n",
        "        )\n",
        "        refreshed = []\n",
        "        extra_visits = 0\n",
        "        updates_detected = 0\n",
        "        for _, path in due_pages:\n",
        "            if extra_visits >= max_visits:\n",
        "                break\n",
        "            state = self.pages[path]\n",
        "            if time.time() - state.last_fetched_ts < self.revisit_window:\n",
        "                continue\n",
        "            _, updated = self.fetch_and_update(path)\n",
        "            refreshed.append(path)\n",
        "            if updated:\n",
        "                updates_detected += 1\n",
        "            extra_visits += 1\n",
        "        return {\n",
        "            \"refreshed_paths\": refreshed,\n",
        "            \"updates_detected\": updates_detected,\n",
        "            \"fetches\": extra_visits,\n",
        "        }\n",
        "\n",
        "    def build_pagerank_matrix(self):\n",
        "        nodes = list(self.graph.keys() | {link for links in self.graph.values() for link in links})\n",
        "        nodes.sort()\n",
        "        node_index = {node: idx for idx, node in enumerate(nodes)}\n",
        "        adjacency = [[] for _ in nodes]\n",
        "        for src, dests in self.graph.items():\n",
        "            if src not in node_index:\n",
        "                continue\n",
        "            src_idx = node_index[src]\n",
        "            adjacency[src_idx] = [node_index[d] for d in dests if d in node_index]\n",
        "        return nodes, adjacency\n",
        "\n",
        "    def pagerank(self, damping: float = 0.85, max_iter: int = 100, tol: float = 1e-6):\n",
        "        nodes, adjacency = self.build_pagerank_matrix()\n",
        "        n = len(nodes)\n",
        "        if n == 0:\n",
        "            return {}\n",
        "        pr = [1.0 / n] * n\n",
        "        teleport = (1.0 - damping) / n\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            new_pr = [teleport] * n\n",
        "            for idx, neighbors in enumerate(adjacency):\n",
        "                if not neighbors:\n",
        "                    share = damping * pr[idx] / n\n",
        "                    for j in range(n):\n",
        "                        new_pr[j] += share\n",
        "                else:\n",
        "                    share = damping * pr[idx] / len(neighbors)\n",
        "                    for dest_idx in neighbors:\n",
        "                        new_pr[dest_idx] += share\n",
        "            delta = sum(abs(new_pr[i] - pr[i]) for i in range(n))\n",
        "            pr = new_pr\n",
        "            if delta < tol:\n",
        "                break\n",
        "        return {nodes[i]: pr[i] for i in range(n)}\n",
        "\n",
        "    def summary(self) -> dict:\n",
        "        total_links = sum(len(state.outgoing) for state in self.pages.values())\n",
        "        unique_pages = len(self.pages)\n",
        "        return {\n",
        "            \"unique_pages\": unique_pages,\n",
        "            \"page_visits\": self.page_visits,\n",
        "            \"node_updates\": self.node_updates,\n",
        "            \"average_out_degree\": (total_links / unique_pages) if unique_pages else 0.0,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17,\n",
              " 17,\n",
              " {'unique_pages': 17,\n",
              "  'page_visits': 17,\n",
              "  'node_updates': 0,\n",
              "  'average_out_degree': 4.0})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "crawler = EfficientCrawler(seed_path=\"/\", revisit_window=5.0)\n",
        "visited = crawler.crawl(max_visits=2000)\n",
        "visited_count = len(visited)\n",
        "initial_summary = crawler.summary()\n",
        "visited_count, crawler.page_visits, initial_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pagerank_scores = crawler.pagerank()\n",
        "len(pagerank_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('/page_6hji76ni', 0.08995015560558127),\n",
              " ('/page_8u5qc2wf', 0.08022155514698967),\n",
              " ('/page_w6ju6bgi', 0.07489186809789804),\n",
              " ('/page_4u9kqiu9', 0.07170398198120716),\n",
              " ('/page_oumbm1ua', 0.06885662863600116),\n",
              " ('/page_zvdv4uqs', 0.067602057849244),\n",
              " ('/page_yn8o949j', 0.06737563261179391),\n",
              " ('/page_7w8neqxl', 0.06049691328827038),\n",
              " ('/page_pjm3eihj', 0.05987092421000326),\n",
              " ('/page_4bfggquc', 0.056975868362335445)]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted_scores = sorted(pagerank_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
        "sorted_scores[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>page_id</th>\n",
              "      <th>last_node_id</th>\n",
              "      <th>last_update</th>\n",
              "      <th>links</th>\n",
              "      <th>history_len</th>\n",
              "      <th>last_fetched_ts</th>\n",
              "      <th>last_changed_ts</th>\n",
              "      <th>updates_detected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/</td>\n",
              "      <td>page_pjm3eihj</td>\n",
              "      <td>9hfmc9s6jcm7</td>\n",
              "      <td>2025-11-15 11:20:09 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/page_13q7cjnj</td>\n",
              "      <td>page_13q7cjnj</td>\n",
              "      <td>lqmtlwgh07pg</td>\n",
              "      <td>2025-11-15 11:20:10 UTC</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>/page_2kghj8tn</td>\n",
              "      <td>page_2kghj8tn</td>\n",
              "      <td>wvy6w4r87xhz</td>\n",
              "      <td>2025-11-15 11:20:05 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>/page_491uh7s9</td>\n",
              "      <td>page_491uh7s9</td>\n",
              "      <td>ctp21ilgds1a</td>\n",
              "      <td>2025-11-15 11:20:05 UTC</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>/page_4bfggquc</td>\n",
              "      <td>page_4bfggquc</td>\n",
              "      <td>3b8kjaw0pn22</td>\n",
              "      <td>2025-11-15 11:19:27 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              path        page_id  last_node_id              last_update  \\\n",
              "0                /  page_pjm3eihj  9hfmc9s6jcm7  2025-11-15 11:20:09 UTC   \n",
              "1   /page_13q7cjnj  page_13q7cjnj  lqmtlwgh07pg  2025-11-15 11:20:10 UTC   \n",
              "6   /page_2kghj8tn  page_2kghj8tn  wvy6w4r87xhz  2025-11-15 11:20:05 UTC   \n",
              "10  /page_491uh7s9  page_491uh7s9  ctp21ilgds1a  2025-11-15 11:20:05 UTC   \n",
              "7   /page_4bfggquc  page_4bfggquc  3b8kjaw0pn22  2025-11-15 11:19:27 UTC   \n",
              "\n",
              "    links  history_len  last_fetched_ts  last_changed_ts  updates_detected  \n",
              "0       5            0     1.763206e+09     1.763206e+09                 0  \n",
              "1       4            0     1.763206e+09     1.763206e+09                 0  \n",
              "6       5            0     1.763206e+09     1.763206e+09                 0  \n",
              "10      4            0     1.763206e+09     1.763206e+09                 0  \n",
              "7       5            0     1.763206e+09     1.763206e+09                 0  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "page_summary = pd.DataFrame([\n",
        "    {\n",
        "        \"path\": path,\n",
        "        \"page_id\": state.page_id,\n",
        "        \"last_node_id\": state.last_node_id,\n",
        "        \"last_update\": state.last_updated_at,\n",
        "        \"links\": len(state.outgoing),\n",
        "        \"history_len\": len(state.history),\n",
        "        \"last_fetched_ts\": state.last_fetched_ts,\n",
        "        \"last_changed_ts\": state.last_changed_ts,\n",
        "        \"updates_detected\": state.updates_detected,\n",
        "    }\n",
        "    for path, state in crawler.pages.items()\n",
        "]).sort_values(\"path\")\n",
        "page_summary.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'results_df' in globals() and not results_df.empty:\n",
        "    results_df.sort_values(\"pagerank\", ascending=False).head(10)[[\"path\", \"pagerank\", \"updates_detected\", \"links\"]]\n",
        "else:\n",
        "    print(\"results_df not yet created. Run the cells above to create page_summary and merge with pagerank_df.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>page_id</th>\n",
              "      <th>last_node_id</th>\n",
              "      <th>last_update</th>\n",
              "      <th>links</th>\n",
              "      <th>history_len</th>\n",
              "      <th>last_fetched_ts</th>\n",
              "      <th>last_changed_ts</th>\n",
              "      <th>updates_detected</th>\n",
              "      <th>pagerank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/</td>\n",
              "      <td>page_pjm3eihj</td>\n",
              "      <td>9hfmc9s6jcm7</td>\n",
              "      <td>2025-11-15 11:20:09 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/page_13q7cjnj</td>\n",
              "      <td>page_13q7cjnj</td>\n",
              "      <td>lqmtlwgh07pg</td>\n",
              "      <td>2025-11-15 11:20:10 UTC</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "      <td>0.054893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/page_2kghj8tn</td>\n",
              "      <td>page_2kghj8tn</td>\n",
              "      <td>wvy6w4r87xhz</td>\n",
              "      <td>2025-11-15 11:20:05 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "      <td>0.044733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/page_491uh7s9</td>\n",
              "      <td>page_491uh7s9</td>\n",
              "      <td>ctp21ilgds1a</td>\n",
              "      <td>2025-11-15 11:20:05 UTC</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "      <td>0.052511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/page_4bfggquc</td>\n",
              "      <td>page_4bfggquc</td>\n",
              "      <td>3b8kjaw0pn22</td>\n",
              "      <td>2025-11-15 11:19:27 UTC</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>1.763206e+09</td>\n",
              "      <td>0</td>\n",
              "      <td>0.056976</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             path        page_id  last_node_id              last_update  \\\n",
              "0               /  page_pjm3eihj  9hfmc9s6jcm7  2025-11-15 11:20:09 UTC   \n",
              "1  /page_13q7cjnj  page_13q7cjnj  lqmtlwgh07pg  2025-11-15 11:20:10 UTC   \n",
              "2  /page_2kghj8tn  page_2kghj8tn  wvy6w4r87xhz  2025-11-15 11:20:05 UTC   \n",
              "3  /page_491uh7s9  page_491uh7s9  ctp21ilgds1a  2025-11-15 11:20:05 UTC   \n",
              "4  /page_4bfggquc  page_4bfggquc  3b8kjaw0pn22  2025-11-15 11:19:27 UTC   \n",
              "\n",
              "   links  history_len  last_fetched_ts  last_changed_ts  updates_detected  \\\n",
              "0      5            0     1.763206e+09     1.763206e+09                 0   \n",
              "1      4            0     1.763206e+09     1.763206e+09                 0   \n",
              "2      5            0     1.763206e+09     1.763206e+09                 0   \n",
              "3      4            0     1.763206e+09     1.763206e+09                 0   \n",
              "4      5            0     1.763206e+09     1.763206e+09                 0   \n",
              "\n",
              "   pagerank  \n",
              "0  0.008824  \n",
              "1  0.054893  \n",
              "2  0.044733  \n",
              "3  0.052511  \n",
              "4  0.056976  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pagerank_df = pd.DataFrame(sorted_scores, columns=[\"path\", \"pagerank\"])\n",
        "results_df = page_summary.merge(pagerank_df, on=\"path\", how=\"left\")\n",
        "results_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refresh Monitoring\n",
        "\n",
        "We periodically revisit pages (respecting the 5-second `revisit_window`) to track node-id churn while keeping the number of extra fetches low.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Submission\n",
        "\n",
        "The assignment requires submitting evaluations to `/evaluate` endpoint:\n",
        "- First evaluation within 15 seconds of first visit\n",
        "- Subsequent evaluations at least every 15 seconds\n",
        "- All evaluations within 60 seconds of first visit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation submission function ready. Use run_evaluation_cycle() to submit evaluations.\n"
          ]
        }
      ],
      "source": [
        "def submit_evaluation(crawler: EfficientCrawler, pagerank_scores: dict) -> dict:\n",
        "    entries = []\n",
        "    for path, state in crawler.pages.items():\n",
        "        page_id = path.lstrip(\"/\")\n",
        "        if not page_id:\n",
        "            page_id = \"/\"\n",
        "        latest_node_id = state.last_node_id\n",
        "        score = pagerank_scores.get(path, 0.0)\n",
        "        entries.append({\n",
        "            \"page_id\": page_id,\n",
        "            \"latest_node_id\": latest_node_id,\n",
        "            \"score\": float(score),\n",
        "        })\n",
        "    \n",
        "    payload = {\"entries\": entries}\n",
        "    try:\n",
        "        response = session.post(\n",
        "            f\"{BASE_URL}/evaluate\",\n",
        "            json=payload,\n",
        "            timeout=5,\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "\n",
        "first_visit_time = None\n",
        "evaluation_results = []\n",
        "\n",
        "def run_evaluation_cycle(crawler: EfficientCrawler, pagerank_scores: dict, start_time: float):\n",
        "    global first_visit_time, evaluation_results\n",
        "    if first_visit_time is None:\n",
        "        first_visit_time = start_time\n",
        "    \n",
        "    elapsed = time.time() - first_visit_time\n",
        "    if elapsed > 60.0:\n",
        "        return False\n",
        "    \n",
        "    result = submit_evaluation(crawler, pagerank_scores)\n",
        "    result[\"elapsed_seconds\"] = elapsed\n",
        "    result[\"visit_count\"] = crawler.page_visits\n",
        "    evaluation_results.append(result)\n",
        "    return elapsed < 60.0\n",
        "\n",
        "\n",
        "print(\"Evaluation submission function ready. Use run_evaluation_cycle() to submit evaluations.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First evaluation: {'coverage': 0.5, 'covered_nodes': 8, 'matched_entries': 8, 'mse': 5.158994034641991e-07, 'total_nodes': 16, 'elapsed_seconds': 15.047605991363525, 'visit_count': 17}\n",
            "Evaluation at 30.05s: MSE=4.250965766518326e-07, Coverage=0.25, Staleness=N/A\n",
            "Evaluation at 45.09s: MSE=2.2128615632037359e-07, Coverage=0.125, Staleness=N/A\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coverage</th>\n",
              "      <th>covered_nodes</th>\n",
              "      <th>matched_entries</th>\n",
              "      <th>mse</th>\n",
              "      <th>total_nodes</th>\n",
              "      <th>elapsed_seconds</th>\n",
              "      <th>visit_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.500</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5.158994e-07</td>\n",
              "      <td>16</td>\n",
              "      <td>15.047606</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.250</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4.250966e-07</td>\n",
              "      <td>16</td>\n",
              "      <td>30.048850</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.125</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.212862e-07</td>\n",
              "      <td>16</td>\n",
              "      <td>45.094120</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   coverage  covered_nodes  matched_entries           mse  total_nodes  \\\n",
              "0     0.500              8                8  5.158994e-07           16   \n",
              "1     0.250              4                4  4.250966e-07           16   \n",
              "2     0.125              2                2  2.212862e-07           16   \n",
              "\n",
              "   elapsed_seconds  visit_count  \n",
              "0        15.047606           17  \n",
              "1        30.048850           17  \n",
              "2        45.094120           17  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "crawler_eval = EfficientCrawler(seed_path=\"/\", revisit_window=5.0)\n",
        "visited_eval = crawler_eval.crawl(max_visits=2000)\n",
        "\n",
        "pagerank_scores_eval = crawler_eval.pagerank()\n",
        "\n",
        "first_visit_time = start_time\n",
        "evaluation_results = []\n",
        "\n",
        "elapsed = time.time() - first_visit_time\n",
        "if elapsed < 15.0:\n",
        "    time.sleep(15.0 - elapsed)\n",
        "\n",
        "result1 = submit_evaluation(crawler_eval, pagerank_scores_eval)\n",
        "result1[\"elapsed_seconds\"] = time.time() - first_visit_time\n",
        "result1[\"visit_count\"] = crawler_eval.page_visits\n",
        "evaluation_results.append(result1)\n",
        "print(f\"First evaluation: {result1}\")\n",
        "\n",
        "last_eval_time = time.time()\n",
        "while time.time() - first_visit_time < 60.0:\n",
        "    time.sleep(15.0)\n",
        "    elapsed = time.time() - first_visit_time\n",
        "    if elapsed >= 60.0:\n",
        "        break\n",
        "    \n",
        "    pagerank_scores_eval = crawler_eval.pagerank()\n",
        "    result = submit_evaluation(crawler_eval, pagerank_scores_eval)\n",
        "    result[\"elapsed_seconds\"] = elapsed\n",
        "    result[\"visit_count\"] = crawler_eval.page_visits\n",
        "    evaluation_results.append(result)\n",
        "    print(f\"Evaluation at {elapsed:.2f}s: MSE={result.get('mse', 'N/A')}, Coverage={result.get('coverage', 'N/A')}, Staleness={result.get('avg_staleness', 'N/A')}\")\n",
        "\n",
        "evaluation_summary = pd.DataFrame(evaluation_results)\n",
        "evaluation_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cycle</th>\n",
              "      <th>refreshed</th>\n",
              "      <th>updates_detected</th>\n",
              "      <th>fetches</th>\n",
              "      <th>total_page_visits</th>\n",
              "      <th>total_updates</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>34</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>51</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>68</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cycle  refreshed  updates_detected  fetches  total_page_visits  \\\n",
              "0      1         17                17       17                 34   \n",
              "1      2         17                 4       17                 51   \n",
              "2      3         17                 4       17                 68   \n",
              "\n",
              "   total_updates  \n",
              "0             17  \n",
              "1             21  \n",
              "2             25  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "monitor_log = []\n",
        "for cycle in range(3):\n",
        "    time.sleep(6)\n",
        "    refresh_stats = crawler.refresh_due_pages(max_visits=20)\n",
        "    summary = crawler.summary()\n",
        "    monitor_log.append({\n",
        "        \"cycle\": cycle + 1,\n",
        "        \"refreshed\": len(refresh_stats[\"refreshed_paths\"]),\n",
        "        \"updates_detected\": refresh_stats[\"updates_detected\"],\n",
        "        \"fetches\": refresh_stats[\"fetches\"],\n",
        "        \"total_page_visits\": summary[\"page_visits\"],\n",
        "        \"total_updates\": summary[\"node_updates\"],\n",
        "    })\n",
        "\n",
        "monitor_df = pd.DataFrame(monitor_log)\n",
        "monitor_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>page_id</th>\n",
              "      <th>last_node_id</th>\n",
              "      <th>updates_detected</th>\n",
              "      <th>last_change_ts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>/page_2kghj8tn</td>\n",
              "      <td>page_2kghj8tn</td>\n",
              "      <td>cawjv4koi6c2</td>\n",
              "      <td>3</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>/page_sbi0db9d</td>\n",
              "      <td>page_sbi0db9d</td>\n",
              "      <td>ncsy2jnnopk4</td>\n",
              "      <td>2</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>/page_zvdv4uqs</td>\n",
              "      <td>page_zvdv4uqs</td>\n",
              "      <td>kuwvlzcoakg2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>/page_6hji76ni</td>\n",
              "      <td>page_6hji76ni</td>\n",
              "      <td>8icwopnhewl2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>/page_491uh7s9</td>\n",
              "      <td>page_491uh7s9</td>\n",
              "      <td>2awa1yw2k2kt</td>\n",
              "      <td>2</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>/page_4u9kqiu9</td>\n",
              "      <td>page_4u9kqiu9</td>\n",
              "      <td>svlvlyhzn5d6</td>\n",
              "      <td>2</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>/page_7w8neqxl</td>\n",
              "      <td>page_7w8neqxl</td>\n",
              "      <td>i4tj8uluw7x7</td>\n",
              "      <td>2</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/</td>\n",
              "      <td>page_pjm3eihj</td>\n",
              "      <td>6vorsjkyvjgb</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/page_13q7cjnj</td>\n",
              "      <td>page_13q7cjnj</td>\n",
              "      <td>b8iu9p8ia5k9</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>/page_4bfggquc</td>\n",
              "      <td>page_4bfggquc</td>\n",
              "      <td>d38oire0nlzq</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/page_8u5qc2wf</td>\n",
              "      <td>page_8u5qc2wf</td>\n",
              "      <td>n59mh7qcllxj</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/page_w6ju6bgi</td>\n",
              "      <td>page_w6ju6bgi</td>\n",
              "      <td>mxvv75f0s3km</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/page_yn8o949j</td>\n",
              "      <td>page_yn8o949j</td>\n",
              "      <td>uzmdenxhydvo</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>/page_pjm3eihj</td>\n",
              "      <td>page_pjm3eihj</td>\n",
              "      <td>6vorsjkyvjgb</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/page_cgwz6tp3</td>\n",
              "      <td>page_cgwz6tp3</td>\n",
              "      <td>s3f6zo8c72e0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>/page_n2l3l0jg</td>\n",
              "      <td>page_n2l3l0jg</td>\n",
              "      <td>h1yfjnvl0vc4</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>/page_oumbm1ua</td>\n",
              "      <td>page_oumbm1ua</td>\n",
              "      <td>rm92azg0rluw</td>\n",
              "      <td>1</td>\n",
              "      <td>1.763206e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              path        page_id  last_node_id  updates_detected  \\\n",
              "6   /page_2kghj8tn  page_2kghj8tn  cawjv4koi6c2                 3   \n",
              "13  /page_sbi0db9d  page_sbi0db9d  ncsy2jnnopk4                 2   \n",
              "5   /page_zvdv4uqs  page_zvdv4uqs  kuwvlzcoakg2                 2   \n",
              "8   /page_6hji76ni  page_6hji76ni  8icwopnhewl2                 2   \n",
              "10  /page_491uh7s9  page_491uh7s9  2awa1yw2k2kt                 2   \n",
              "11  /page_4u9kqiu9  page_4u9kqiu9  svlvlyhzn5d6                 2   \n",
              "15  /page_7w8neqxl  page_7w8neqxl  i4tj8uluw7x7                 2   \n",
              "0                /  page_pjm3eihj  6vorsjkyvjgb                 1   \n",
              "1   /page_13q7cjnj  page_13q7cjnj  b8iu9p8ia5k9                 1   \n",
              "7   /page_4bfggquc  page_4bfggquc  d38oire0nlzq                 1   \n",
              "2   /page_8u5qc2wf  page_8u5qc2wf  n59mh7qcllxj                 1   \n",
              "3   /page_w6ju6bgi  page_w6ju6bgi  mxvv75f0s3km                 1   \n",
              "4   /page_yn8o949j  page_yn8o949j  uzmdenxhydvo                 1   \n",
              "12  /page_pjm3eihj  page_pjm3eihj  6vorsjkyvjgb                 1   \n",
              "9   /page_cgwz6tp3  page_cgwz6tp3  s3f6zo8c72e0                 1   \n",
              "14  /page_n2l3l0jg  page_n2l3l0jg  h1yfjnvl0vc4                 1   \n",
              "16  /page_oumbm1ua  page_oumbm1ua  rm92azg0rluw                 1   \n",
              "\n",
              "    last_change_ts  \n",
              "6     1.763206e+09  \n",
              "13    1.763206e+09  \n",
              "5     1.763206e+09  \n",
              "8     1.763206e+09  \n",
              "10    1.763206e+09  \n",
              "11    1.763206e+09  \n",
              "15    1.763206e+09  \n",
              "0     1.763206e+09  \n",
              "1     1.763206e+09  \n",
              "7     1.763206e+09  \n",
              "2     1.763206e+09  \n",
              "3     1.763206e+09  \n",
              "4     1.763206e+09  \n",
              "12    1.763206e+09  \n",
              "9     1.763206e+09  \n",
              "14    1.763206e+09  \n",
              "16    1.763206e+09  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "updated_pages = [\n",
        "    {\n",
        "        \"path\": path,\n",
        "        \"page_id\": state.page_id,\n",
        "        \"last_node_id\": state.last_node_id,\n",
        "        \"updates_detected\": state.updates_detected,\n",
        "        \"last_change_ts\": state.last_changed_ts,\n",
        "    }\n",
        "    for path, state in crawler.pages.items()\n",
        "    if state.updates_detected > 0\n",
        "]\n",
        "\n",
        "updated_pages_df = pd.DataFrame(updated_pages)\n",
        "updated_pages_df.sort_values(\"updates_detected\", ascending=False) if not updated_pages_df.empty else \"No node-id updates observed during monitoring window.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'unique_pages': 17,\n",
              " 'page_visits': 68,\n",
              " 'node_updates': 25,\n",
              " 'average_out_degree': 4.0}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_summary = crawler.summary()\n",
        "final_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'due_count': 0, 'sample': []}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def pages_due_for_refresh(crawler: EfficientCrawler, horizon: float = 120.0):\n",
        "    now = time.time()\n",
        "    due = []\n",
        "    for path, state in crawler.pages.items():\n",
        "        if now - state.last_fetched_ts >= horizon:\n",
        "            due.append(path)\n",
        "    return due\n",
        "\n",
        "pending_refresh = pages_due_for_refresh(crawler, horizon=10.0)\n",
        "{\"due_count\": len(pending_refresh), \"sample\": pending_refresh[:5]}\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
